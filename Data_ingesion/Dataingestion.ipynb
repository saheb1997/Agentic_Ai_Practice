{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc00675",
   "metadata": {},
   "source": [
    "## Data ingestion -Documention\n",
    "https://python.langchain.com/docs/integrations/document_loaders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40afeb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x24830deb850>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader('spech.txt')\n",
    "loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bec13ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'spech.txt'}, page_content='Artificial Intelligence (AI) is a branch of computer science focused on creating systems capable of performing tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, perception, language understanding, and decision-making.\\n\\nAI can be categorized into two main types: narrow AI and general AI. Narrow AI, also known as weak AI, is designed to perform a specific task, such as voice recognition or image classification. General AI, or strong AI, refers to systems that possess the ability to understand, learn, and apply knowledge across a wide range of tasks, similar to human intelligence. However, general AI remains largely theoretical at this stage.\\n\\nMachine learning (ML) is a subset of AI that enables systems to learn from data and improve their performance over time without being explicitly programmed. ML algorithms identify patterns in data and use these patterns to make predictions or decisions. Deep learning, a further subset of ML, uses artificial neural networks with many layers to model complex patterns in large datasets.\\n\\nNatural language processing (NLP) is another important area of AI. NLP enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, language translation, sentiment analysis, and speech recognition.\\n\\nComputer vision is a field of AI that allows machines to interpret and understand visual information from the world, such as images and videos. Computer vision is used in facial recognition, autonomous vehicles, medical imaging, and more.\\n\\nAI has numerous real-world applications. In healthcare, AI assists in diagnosing diseases, personalizing treatment plans, and managing patient data. In finance, AI is used for fraud detection, algorithmic trading, and customer service automation. In manufacturing, AI optimizes production processes and predictive maintenance.\\n\\nAutonomous vehicles rely heavily on AI to perceive their environment, make decisions, and navigate safely. AI-powered virtual assistants, such as Siri, Alexa, and Google Assistant, help users manage tasks, answer questions, and control smart devices.\\n\\nAI also plays a significant role in entertainment, powering recommendation systems for music, movies, and online content. In education, AI personalizes learning experiences and automates administrative tasks.\\n\\nDespite its many benefits, AI presents challenges and ethical concerns. Issues such as data privacy, algorithmic bias, job displacement, and the potential misuse of AI technologies require careful consideration and regulation.\\n\\nAI systems are only as good as the data they are trained on. Poor-quality or biased data can lead to inaccurate or unfair outcomes. Transparency and explainability in AI decision-making are important for building trust and accountability.\\n\\nThe development of AI is driven by advances in hardware, such as powerful GPUs, and the availability of large datasets. Open-source frameworks like TensorFlow and PyTorch have made AI research and development more accessible.\\n\\nAI research continues to push the boundaries of what machines can do. Areas of active research include reinforcement learning, transfer learning, generative models, and explainable AI.\\n\\nReinforcement learning enables agents to learn optimal actions through trial and error, receiving feedback in the form of rewards or penalties. This approach has been used to train AI systems to play games, control robots, and optimize resource allocation.\\n\\nGenerative models, such as Generative Adversarial Networks (GANs) and large language models, can create new content, including images, music, and text. These models have applications in creative industries, data augmentation, and simulation.\\n\\nExplainable AI aims to make AI systems more transparent and understandable to humans. This is crucial for applications in sensitive domains like healthcare and law, where understanding the reasoning behind AI decisions is essential.\\n\\nAI is transforming industries and society, offering new opportunities and challenges. As AI technologies continue to evolve, it is important to ensure they are developed and used responsibly, with consideration for ethical, legal, and social implications.\\n\\nThe future of AI holds great promise, with the potential to solve complex problems, improve quality of life, and drive economic growth. Collaboration between researchers, policymakers, and industry stakeholders is essential to harness the benefits of AI while mitigating its risks.\\n\\nIn summary, AI is a rapidly advancing field with the potential to revolutionize many aspects of our lives. Its continued development will depend on responsible innovation, ethical considerations, and a commitment to using technology for the greater good.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_document=loader.load()\n",
    "text_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36143927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 0, 'page_label': '791'}, page_content='Vol.:(0123456789)\\nAnnals of Operations Research (2022) 319:791–822\\nhttps://doi.org/10.1007/s10479-020-03514-x\\n1 3\\nS.I. : DESIGN AND\\xa0MANAGEMENT OF\\xa0HUMANITARIAN SUPPLY \\nCHAINS\\nA deep multi‑modal neural network for\\xa0informative Twitter \\ncontent classification during\\xa0emergencies\\nAbhinav\\xa0Kumar1\\xa0· Jyoti\\xa0Prakash\\xa0Singh1\\xa0· Yogesh\\xa0K.\\xa0Dwivedi2\\xa0· Nripendra\\xa0P .\\xa0Rana3\\nPublished online: 16 January 2020 \\n© Springer Science+Business Media, LLC, part of Springer Nature 2020\\nAbstract\\nPeople start posting tweets containing texts, images, and videos as soon as a disaster hits an \\narea. The analysis of these disaster-related tweet texts, images, and videos can help human-\\nitarian response organizations in better decision-making and prioritizing their tasks. Find-\\ning the informative contents which can help in decision making out of the massive volume \\nof Twitter content is a difficult task and require a system to filter out the informative con-\\ntents. In this paper, we present a multi-modal approach to identify disaster-related informa-\\ntive content from the Twitter streams using text and images together. Our approach is based \\non long-short-term-memory and VGG-16 networks that show significant improvement in \\nthe performance, as evident from the validation result on seven different disaster-related \\ndatasets. The range of F1-score varied from 0.74 to 0.93 when tweet texts and images used \\ntogether, whereas, in the case of only tweet text, it varies from 0.61 to 0.92. From this \\nresult, it is evident that the proposed multi-modal system is performing significantly well in \\nidentifying disaster-related informative social media contents.\\nKeywords Disaster\\xa0· Twitter\\xa0· LSTM\\xa0· VGG-16\\xa0· Social media\\xa0· Tweets\\n * Yogesh K. Dwivedi \\n ykdwivedi@gmail.com\\n Abhinav Kumar \\n abhinavanand05@gmail.com\\n Jyoti Prakash Singh \\n jps@nitp.ac.in\\n Nripendra P. Rana \\n nrananp@gmail.com\\n1 Department of\\xa0Computer Science and\\xa0Engineering, National Institute of\\xa0Technology Patna, Patna, \\nIndia\\n2 School of\\xa0Management, Emerging Markets Research Centre (EMaRC), Swansea University Bay \\nCampus, Fabian Way, Swansea\\xa0SA1\\xa08EN, UK\\n3 School of\\xa0Management, University of\\xa0Bradford, Richmond Rd, Bradford\\xa0BD7\\xa01DP, UK'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 1, 'page_label': '792'}, page_content='792 Annals of Operations Research (2022) 319:791–822\\n1 3\\n1 Introduction\\nA natural disaster creates significant ecological disruption requiring extensive efforts from \\nsociety to overcome and cope with them (Imran et\\xa0 al. 2015; Sakaki et\\xa0 al. 2013; Kumar \\nand Singh 2019). In the case of natural or man-made disasters, rescue organizations need \\nto respond to all the affected people on time. However, this task is very challenging to the \\nprofessional humanitarian communities and government agencies due to the limited infor -\\nmation of the victims’ location, massive number of calls by victims and their relatives, and \\nprioritizing rescue operations based on the need of victims (Imran et\\xa0al. 2015; Sakaki et\\xa0al. \\n2013; Kumar and Singh 2019; Kumar et\\xa0al. 2019; John et\\xa0al. 2018; Jin et\\xa0al. 2015; Paul and \\nHariharan 2012; Dubey et\\xa0al. 2017; Shareef et\\xa0al. 2018; Sinha et\\xa0al. 2019; Nguyen et\\xa0al. \\n2016). The lack of coordination among rescue organizations and supply chain actors results \\nin significant financial and life loss (Dubey et\\xa0al. 2014, 2019; Dwivedi et\\xa0al. 2018; Jabbour \\net\\xa0al. 2017). On average, 388 disasters have occurred annually from 2003 to 2012, causing \\nan economic damage worth of 156.7 billion US dollars (Guha-Sapir et\\xa0al. 2012). It is found \\nthat during an emergency, a massive amount of user-generated data is posted on social \\nmedia platforms such as Twitter and Facebook (Kapoor et\\xa0al. 2018; Kim and Hastak 2018; \\nRagini et\\xa0al. 2018; Son et\\xa0al. 2019). These social media platforms are used by the people \\nto communicate at different levels, such as from person to person, person to government \\nagencies, and government to people (Alalwan et\\xa0al. 2017; Dwivedi et\\xa0al. 2015; Elbanna \\net\\xa0al. 2019; Jamali et\\xa0al. 2019; Kim et\\xa0al. 2018; Kumar and Singh 2019; Nguyen et\\xa0al. 2016; \\nKumar et\\xa0al. 2017; Singh et\\xa0al. 2019; Zhang et\\xa0al. 2019). Victims and eyewitnesses often \\npost their status; report infrastructure damage; inform about injured people; and also ask \\nfor help through these platforms with text, images, and videos. These user-generated data \\nproduced through social networking sites are pervasive, rapid, and accessible that can be \\nused to coordinate for helping the victims and empowering citizens to become more situa-\\ntionally aware at the time of disaster (Dubey 2019; Caragea et\\xa0al. 2016; Papadopoulos et\\xa0al. \\n2017; Akter and Wamba 2017). Several examples are evidence where social media has \\nplayed a vital role in relief efforts, finding help, and potentially saving lives. For instance, \\nin the case of Hurricane Harvey, a woman was rescued when she tweeted for help as the \\nemergency contact number “911” was not reachable. 1 In the case of the Chennai flood in \\nIndia, people asked for help by posting their message on Twitter (Singh et\\xa0al. 2019).\\nAmong the massive volume of tweets related to a disaster, some of them might be \\njust thanking Twitter or local groups for their help. These tweets are not very useful for a \\nhumanitarian organization in their rescue work. These types of tweets are termed as non-\\ninformative tweets. The other types of tweets where people are asking for help, locating \\ntheir relatives, provide information regarding infrastructure and utility damage, affected \\nindividuals, injured or dead people. These types of tweets are termed as informative tweets. \\nIt is impossible for emergency responders to manually go through each of the posts to mine \\ninformative posts to take action due to the massive volume and speed of tweets posting. \\nThis manual inspection can also take away valuable human resources from other essential \\ntasks. Therefore, this creates an immediate need to build systems that can automatically \\nfilter the informative contents out of a large volume of social media content. The automatic \\nclassification of social media messages, especially tweet texts, is a challenging task due to \\ntheir limitation in size (only 280 characters), non-standard abbreviations, and grammatical \\n1 http://time.com/49219 61/hurri cane-harve y-twitt er-faceb ook-socia l-media /.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 2, 'page_label': '793'}, page_content='793Annals of Operations Research (2022) 319:791–822 \\n1 3\\nerrors (Nguyen et\\xa0al. 2017a, b, c; Imran et\\xa0al. 2015). Recent works by Caragea et\\xa0al. (2016) \\nand Nguyen et\\xa0al. (2016, 2017a) explored tweet texts only to filter disaster-related informa-\\ntive tweets from social media. But people also post a good volume of images and videos \\nrelated to disaster, which can give a lot of insight into the event. A few recent works by \\nAlam et\\xa0al. (2017) and Nguyen et\\xa0al. (2017b) used visual features only in finding informa-\\ntive images in case of disaster. Rizk et\\xa0al. (2019) and Mouzannar et\\xa0al. (2018) used both \\ntextual and visual features related to build-infrastructure damage, nature damage, and fire \\nfor estimating the damage due to disaster. To the best of our knowledge, no work has been \\nreported where tweet text and images are used together to filter informative tweets from \\nmassive social media contents.\\nThe need for a robust disaster-related informative tweet filtering system has motivated \\nus to build a multi-modal system that uses tweet text and images to filter out informative \\nand non-informative posts. The proposed model uses long-short-term-memory (LSTM) for \\ntweet text and convolutional neural network (CNN)-based VGG-16 network for images. \\nWe used deep neural network because conventional classification methods require manu-\\nally engineered features such as TF-IDF vectors, clue words, and Bag-of-Visual-Words. \\nThe performance of these conventional classifiers depends heavily on how efficiently the \\nfeatures were extracted. The deep neural network models are better suited for the classifica-\\ntion of the disaster-related data than traditional classification approach because they learn \\nfeatures automatically (Nguyen et\\xa0al. 2016). The proposed model is evaluated on datasets \\nof seven different disasters: Hurricane Harvey, Hurricane Maria, Hurricane Irma, Califor -\\nnia wildfires, Iran–Iraq earthquake, Mexico earthquake, and Sri Lanka Flood. The contri-\\nbution of this paper can be summarized below:\\n1. Development of a multi-modal system to classify informative and non-informative \\ntweets containing either text, image, or both together.\\n2. Eliminating the need for feature engineering using LSTM and CNN for text and images \\nrespectively to extract relevant features.\\n3. Evaluating the effect of texts and images in the classification of informative contents.\\n4. Validating the model with cross-event disaster-related datasets to see their efficiency in \\nthe early stage of the cross-event disaster.\\nThe rest of the paper is organized as follows: Sect.\\xa0 2 discusses the related works; the \\ndetailed description of the methodology is discussed in Sect.\\xa0 3. The findings of the experi-\\nmentation are listed in Sect.\\xa0 4. Section\\xa05 discusses the overall findings, theoretical contri-\\nbutions, and practical implications of the current work. We conclude the paper in Sect.\\xa06.\\n2  Related literature\\nRecently, several works (Imran et\\xa0al. 2015; Atefeh and Khreich 2015; Zheng et\\xa0al. 2018) \\nhave been reported for efficiently utilizing the disaster-related social media data for situ-\\national awareness. Finding informative contents from the massive social media data is one \\nof the essential tasks for humanitarian organizations. A number of works (Caragea et\\xa0al. \\n2016; Nguyen et\\xa0al. 2016, 2017a; Imran et\\xa0al. 2014; Yu et\\xa0al. 2019; Ashktorab et\\xa0al. 2014; \\nAlam et\\xa0al. 2017; Daly and Thom 2016) have been reported to identify informative social \\nmedia contents. However, most of the work focused on the social media text only, whereas \\nimages get very little attention in finding informative content. This section is divided into'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 3, 'page_label': '794'}, page_content='794 Annals of Operations Research (2022) 319:791–822\\n1 3\\ntwo subsections for better organization of the related literatures: (i) Informative social \\nmedia text classification and (ii) Informative social media image classification.\\n2.1  Informative social media text classification\\nThe deep neural network-based model is used by Nguyen et\\xa0 al. (2016) to classify mes-\\nsages into informative and not-informative classes. They are further classified informa-\\ntive messages into different classes such as affected individuals, infrastructure and utility \\ndamage, and sympathy and support. Caragea et\\xa0al. (2016) proposed a convolutional neu-\\nral network-based model to classify tweets into informative and not-informative classes. \\nTheir model showed significant improvement over other models that uses n-gram features \\non flooding datasets. They got their best result of 82.52% in the case of CNN, where they \\nused the Philippines, Colorado, and Queensland floods datasets together as training and \\nManila floods dataset as testing. Nguyen et\\xa0 al. (2017a) proposed a convolutional neural \\nnetwork-based model to classify tweets into informative and not-informative classes. They \\nshowed out-of-event data could be considered for training the classifier in the early stage \\nof the events for reducing the effect of the cold-start problem. Caragea et\\xa0al. (2011) used \\nkeyword-based classification and SVM techniques to classify Haiti earthquake tweets into \\nthe multi-label setting. They considered several classes such as medical emergency, food \\nshortage, hospital/clinic services in their analysis and achieved F1-scores of 0.47 and 0.59 \\nfor the keyword-based classification and SVM, respectively. Imran et\\xa0al. (2014) developed \\nan Artificial Intelligence for Disaster Response (AIDR) platform to classify Twitter mes-\\nsages into the user-defined classes in real-time. They used human and machine intelligence \\nfor labeling a subset of disaster-related messages and trained the model to classify new \\nmessages automatically. They tested their platform with the Pakistan earthquake (2013), \\nwhere they classified messages into informative and not-informative classes with an AUC \\nof 80%. Aipe et\\xa0al. (2018) developed a deep Convolutional Neural Network (CNN)-based \\nmodel for multi-label classification of crisis-related tweets. They also explored the uses of \\nTwitter-centric textual features such as hashtags, user-mentions, and keywords extracted \\nfrom the URLs in the classification task. They found the positive influence of the Twitter-\\ncentric features on the performance of the classifier. Their model achieved F1-scores of \\n0.75 to 0.98 for the seven different categories, such as Casualties and Public Impact, Col-\\nlateral Damages, General Awareness, Voluntary Services, Sympathy and Emotion, Crisis-\\nspecific Information, and Non-informative. Yu et\\xa0 al. (2019) used CNN, support vector \\nmachine (SVM), and logistic regression (LR) to classify tweets related to Hurricane Sandy, \\nHurricane Harvey, and Hurricane Irma. They classified tweets into different classes such \\nas Caution and Advice, Information Sources, Casualties and Damage, Infrastructure and \\nResources, and Donation and Aid. They tested their model with two different settings (i) \\nevent-specific data and (ii) out-of-event data and achieved F1-score in the range of 0.31 \\nto 0.80. Their CNN-based model performed best in comparison of SVM and LR. Huang \\nand Xiao (2015) manually examined several tweets related to hurricane sandy and code \\nthem into different themes. They then used a logistic regression classifier to the tweets to \\nachieve an average F1-score of 0.66. Ashktorab et\\xa0al. (2014) used several machine-learning \\ntechniques such as SVM, logistic regression, Naive Bayes, decision tree, KNN, and super -\\nvised latent Dirichlet allocation to identify tweets reporting to damage or casualties. They \\nfound their best result in the case of logistic regression with an F1-score of 0.65. Imran \\net\\xa0al. (2013b) used informative messages posted during Joplin 2011 and Sandy 2012, and \\nthen they used a model based on conditional random fields to extract valuable information'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 4, 'page_label': '795'}, page_content='795Annals of Operations Research (2022) 319:791–822 \\n1 3\\nfrom those informative tweets. They achieved the detection rate of 25% to 91% when tested \\nwith the event-specific dataset and 1% to 49% when they trained and tested their model \\nwith the combination of both Joplin 2011 and Sandy 2012. Imran et\\xa0al. (2013a) performed \\nthree tasks: (i) classified tweets into informative, personal, and others classes; (ii) classified \\ninformative tweets into different classes such as Caution, Donation, Casualty, and Informa-\\ntion Source; and (iii) extracted several information from the informative tweets such as \\nLocation references, Source, and Type of Caution. They used several textual features from \\nthe tweet and used the Naive Bayes classifier for both classification task, whereas they used \\nStanford Named Entity Recognizer to extract information from informative tweets. They \\ngot AUC of 0.828 in finding informative tweets and got an F1-score of 0.562 to 0.809 \\nin classifying those informative tweets into further classes. Their information extraction \\nmodel achieved a precision of 0.47 to 0.93 in finding various information nuggets. Olteanu \\net\\xa0al. (2014) created a lexicon of frequently appearing crisis-related terms in the relevant \\nmessages. They used this lexicon to automatically identify new terms for a given crisis and \\nquery Twitter API to extract crisis-related messages. Graf et\\xa0al. (2018) extracted linguis-\\ntic, emotional, and sentimental features from the disaster-related messages and developed a \\ncross-domain classifier. They performed extensive experiments with 26 different disaster-\\nrelated datasets. They found their best result with an average accuracy of 80% in the case of \\ncross-domain classification, where they used 25 datasets for training and the remaining one \\nfor testing. Li et\\xa0al. (2015) applied the Naive Bayes classifier on the Hurricane Sandy and \\nBoston Marathon bombing Twitter data to study the applicability of domain adaption for \\nmining disaster-related tweets. Rudra et\\xa0al. (2016) developed a framework to classify Nepal \\nEarthquake tweets into different classes. They summarize those classified tweets to gener -\\nate comprehensive abstractive summaries. Cameron et\\xa0al. (2012) developed the Emergency \\nSituation Awareness-Automated Web Text Mining (ESA-AWTM) system that identifies \\nthe relevant Twitter messages. Then those relevant messages are used to inform situation \\nawareness of the disaster-related incidents. Verma et\\xa0al. (2011) build a classifier that used \\nautomatically extracted linguistic features to categorize tweets. Their system achieved over \\n80% in classifying the tweets to contribute the situational awareness. The survey regarding \\nthe processing of social media messages and their contributions to situation awareness can \\nbe seen in Imran et\\xa0al. (2015). Some of the potential work which uses social media texts for \\nthe classification task are listed in Table\\xa01.\\n2.2  Informative social media image classification\\nThe Image4Act framework is developed by Alam et\\xa0 al. (2017) for identifying relevant \\nimages posted on the social media platform to help humanitarian organizations. They \\ntested their framework for the Queensland Australian Cyclone, 2017, and achieved the pre-\\ncision of 0.67 and 0.92 in finding relevant and duplicate images, respectively. Nguyen et\\xa0al. \\n(2017b) developed a pipeline to detect irrelevant and redundant images during a disaster \\nfrom social media streams. The detection of irrelevant images is done using a transfer-\\nlearning approach based on deep neural networks. For the detection of redundant images, \\nthey used perceptual hashing techniques. Chaudhuri and Bose (2019) used earthquake-\\nrelated images and applied the convolutional neural network to identify the human body \\npart from the debris and achieved an accuracy of 83.2%. Daly and Thom (2016) used \\nFlicker images and extracted features from the images to detect the fire event. They found \\na recall of 91% and a precision of 93% in detecting fire from the images. Lagerstrom et\\xa0al. \\n(2016) used bush fire-related images of the Australian state of NSW and classified them'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 5, 'page_label': '796'}, page_content='796 Annals of Operations Research (2022) 319:791–822\\n1 3\\nTable 1  List of some potential works for the classification of social media text\\nAuthor Task Techniques Data Evaluation Metrics\\nAccuracy AUC F1-score\\nCaragea et\\xa0al. (2016) Informative versus Not-inform-\\native\\nCNN Philippines floods (2012), Colo-\\nrado floods (2013), etc.\\n75.90–82.52 – –\\nNguyen et\\xa0al. (2016) Informative versus Not-informa-\\ntive and others\\nCNN Nepal Earthquake and others – 67–78 –\\nNguyen et\\xa0al. (2017) Informative versus Not-inform-\\native\\nSupport vector Machine, Logistic \\nRegression, Random Forest and \\nCNN\\nNepal Earthquake, California \\nEarthquake, Cyclone, etc.\\n– 50.12–94.17 –\\nImran et\\xa0al. (2014) Informative versus Not-inform-\\native\\n– Pakistan Earthquake, 2013 – 80 –\\nYu et\\xa0al. (2019) Caution and Advice, Casualties \\nand Damage, Infrastructure and \\nResources, etc.\\nCNN, Support vector machine, \\nLogistic regression\\nHurricane Sandy, Hurricane Har-\\nvey, and Hurricane Irma\\n– – 0.31–0.80\\nHuang and Xiao (2015) Relief, Utility recovery, etc. Logistic regression Hurricane Sandy – – 0.00–0.92\\nAshktorab et\\xa0al. (2014) Damage or casualties versus \\nothers\\nLogistic regression, SVM, KNN, \\netc.\\nChristchurch earthquake, Hur-\\nricane, Tornado, etc.\\n70.0–86.0 69–88 0.50–0.65\\nAipe et\\xa0al. (2018) General Awareness, Sympathy and \\nEmotion, Non-informative, etc.\\nCNN California Earthquake, Nepal \\nEarthquake, India Flood, etc.\\n– – 0.75–0.98\\nCaragea et\\xa0al. (2011) Medical emergency, food short-\\nage, hospital/clinic services, etc.\\nKeyword-based classification, \\nSVM\\nHaiti earthquake – – 0.47–0.59'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 6, 'page_label': '797'}, page_content='797Annals of Operations Research (2022) 319:791–822 \\n1 3\\ninto the fire and not-fire classes with an accuracy of 86%. Nguyen et\\xa0al. (2017c) used a \\ndeep convolutional neural network for classifying disaster-related social media images into \\nsevere, mild, and no-damage classes to analyze the impact of the disaster. They used Nepal \\nEarthquake, Ecuador Earthquake, Hurricane Matthew, Typhoon Ruby, and Google Images \\ndatasets and trained event-specific as well as cross-event classifier. Their CNN model out-\\nperformed Bag-of-Visual-Words (BoVW) techniques and achieved the F1-scores in the \\nrange of 0.67 to 0.89.\\nRecently, researchers have proposed multi-modal systems utilizing the tweet text and \\nimages both for finding relevant information from social media. Rizk et\\xa0al. (2019) devel-\\noped a multi-modal disaster-related classifier to classify Twitter data into the built-infra-\\nstructure damage and nature damage classes. They concatenated semantic features from \\ntweet text and visual features from the image and achieved an accuracy of 92.43%, whereas \\na model that uses only visual features achieved an accuracy of 91.10%. Mouzannar et\\xa0al. \\n(2018) developed a multi-modal system based on the deep-learning framework to classify \\nusers post into Fire, Floods, Natural landscape damage, Infrastructural damage, Injuries \\nand dead people, and Non-damage classes. They used CNN-based Inception model for \\nimage and CNN model for text and combined textual and visual features to classify users’ \\nposts and achieved accuracy of 92.62%.\\nThe recently developed multi-modal system is focused on classifying the social media \\ncontents into various damage related classes such as build-infrastructure damage, natural \\ndamage, and non-damage. None of the works utilized images with the tweet text in finding \\ninformative content from the massive social media contents. In this work, we are extracting \\nfeatures from images and combined these features with the features extracted from tweet \\ntext to investigate the role of images in finding informative Twitter contents in the case of \\nthe disaster.\\n3  Methodology\\nThe overall architecture of the proposed multi-modal system is shown in Fig.\\xa0 3. The sys-\\ntem consists of two parallel deep neural architectures: (i) long-short-term-memory (LSTM) \\nnetwork for processing textual data and (ii) VGG-16 network for processing images. The \\ntweet text is embedded into a vector form using an Embedding layer shown at the upper \\nleft part of Fig.\\xa0 3. This embedded tweet text is then passed through two LSTM layers to \\nextract features from them, which is used to classify the tweet text into informative or \\nnot-informative classes. For the image, Convolutional Neural Network (CNN)-based pre-\\ntrained VGG-16 model is used to extract features from them. All the weights of the VGG-\\n16 network are marked as non-trainable except the weight between the last dense layer and \\nthe output layer. For the multi-modal setting, the feature vector coming from the second \\nlast dense layer of VGG-16 is passed through another dense layer containing 256 neurons, \\nas shown in Fig.\\xa03. This 256-dimensional feature vector coming from VGG-16 is then con-\\ncatenated with the 256-dimensional tweet text feature coming from the last LSTM layer to \\nmake a 512-dimensional combined feature vector. This 512-dimensional feature vector is \\nthen used to predict informative and not-informative Twitter contents. In the following sub-\\nsections, we will describe the data pre-processing, image classification, text classification, \\nmultimodal system, and majority voting scheme: (i) data description and pre-processing, \\n(ii) image classification (VGG-16), (iii) tweet text classification (long-short-term-mem-\\nory), (iv) multi-modal system (VGG-16 + LSTM), and (v) majority voting.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 7, 'page_label': '798'}, page_content='798 Annals of Operations Research (2022) 319:791–822\\n1 3\\n3.1  Data description and\\xa0pre‑processing\\nThe current research uses the dataset published by Alam et\\xa0al. (2018) to validate the pro-\\nposed system. It contains seven different disaster-related datasets: (i) Hurricane Harvey, (ii) \\nHurricane Maria, (iii) Hurricane Irma, (iv) Mexico earthquake, (v) Iran–Iraq earthquake, \\n(vi) California Wildfire, and (vii) Sri Lanka flood. The detailed description regarding the \\ntime period and keywords used in the collection for the datasets can be seen in Alam et\\xa0al. \\n(2018). Here, we are listing the definition of each of the classes mentioned in the datasets \\nwhich we will use to validate the proposed system: (i) informative: if the tweet/image is \\nuseful for humanitarian aid, (ii) not informative: if the tweet/image is not useful for human-\\nitarian aid. Some sample tweets with images for informative and not-informative classes \\nare shown in Figs.\\xa01 and 2, respectively. During the creation of the dataset, if a tweet con-\\ntains more than one image URL, then all the images are downloaded and used the same \\ntweet text with all corresponding images. That means the dataset contains duplicate tweet \\ntext in several cases. The data sample containing duplicate tweet text is removed. Finally, \\nwe randomly took an equal sample of informative and not-informative tweet text for further \\nprocessing. The detail description of the datasets is shown in Table\\xa02. In case of tweet texts, \\nsymbols such as “#,” “@,” “!,” “&,” and “%” do not contribute to the classification task, \\nFig. 1  Sample informative images with tweets\\nFig. 2  Sample not-informative images with tweets'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 8, 'page_label': '799'}, page_content='799Annals of Operations Research (2022) 319:791–822 \\n1 3\\nTable 2  Number of informative (Info) and not-informative (Not-info) data samples for different disasters\\nHurricane Harvey \\n(1940)\\nHurricane Maria \\n(2972)\\nHurricane Irma \\n(1672)\\nMexico Earthquake \\n(624)\\nIran–Iraq Earthquake \\n(168)\\nCalifornia Wildfires \\n(648)\\nSri Lanka Flood \\n(572)\\nInfo Not-info Info Not-info Info Not-info Info Not-info Info Not-info Info Not-info Info Not-info\\nTweet text 970 970 1486 1486 836 836 312 312 84 84 324 324 286 286\\nImage 905 1035 1423 1549 701 971 285 339 64 104 328 320 187 385'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 9, 'page_label': '800'}, page_content='800 Annals of Operations Research (2022) 319:791–822\\n1 3\\nso these are removed from the dataset, and all the tweet texts are converted into the lower \\ncase. In the case of images, all the images are converted into equal sizes of (224 × 224 × 3) . \\nTo do the normalization, the pixel matrix of the image is divided by the maximum value, \\ni.e., 255. The normalized matrix is then used by the proposed system to train and test the \\nmodel. In all the cases, out of the total data sample, 75% of them were used for training, \\nand the remaining 25% sample was used for testing the performance of the models.\\n3.2  Image classification (VGG‑16)\\nVGG-16 is a deep convolutional neural network architecture designed to classify ImageNet \\ndatasets into the 1000 classes. It consists of 13 convolutional layers, followed by three fully \\nconnected layers. It takes an image size of (224 × 224 × 3) as an input and performs con-\\nvolution operation using a (3 × 3) filter. The detailed description regarding the layers and \\nparameters of the VGG-16 network can be seen in Simonyan and Zisserman (2014). The \\nuniform architecture of VGG-16 is very appealing, and currently, it is considered as the \\nmost preferred choice for extracting features from images. This VGG-16 network is proved \\nto be effective for the number of image classification tasks (Nguyen et\\xa0al. 2017a, b, c; Alam \\net\\xa0al. 2017; Simonyan and Zisserman 2014). Due to the diverse applications of the VGG-\\n16 model for various image processing tasks, the proposed work uses this network. The \\nlast layer of the network can be adapted according to the type of classification. As our \\ncase is related to binary classification, two neurons are used at the output layer, one for the \\ninformative and another one for the not-informative class. The overall architecture of the \\nVGG-16 model can be seen in Fig.\\xa0 3. The weights of the VGG-16 model up to the second \\ndense layer are marked as non-trainable, which is represented in Fig.\\xa03 by a dotted box. The \\nweights between the second last dense layer and output layers are trained by passing the \\nFig. 3  Proposed multi-modal neural network model for the classification of Twitter Contents'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 10, 'page_label': '801'}, page_content='801Annals of Operations Research (2022) 319:791–822 \\n1 3\\nimage through the network. The model uses softmax activation function with categorical \\ncross-entropy as a loss function, which can be defined by Eqs.\\xa0(1) and (2), respectively.\\nwhere xi is the numerical value coming at the output layer from its previous layer and M \\nrepresents the number of classes. The softmax function is calculating the probabilities of \\neach target class over all possible target classes. In the second equation, yi represents one-\\nhot vector for the number of classes and ̂yi represents the predicted class probability of \\nthe model for the ith training sample in a batch of N training sample. In the convolutional \\nlayers of VGG-16, Rectified Linear Unit (ReLU) (Nair and Hinton 2010) is used as an acti-\\nvation function. The ReLu function is defined as: f(x) = max (0,x), it means for all values \\nof x < 0 it return 0 and for x > 0 it return x itself. The model uses Adam (Kingma and Ba \\n2014) as the optimizer. The hyper-parameters used in this study are listed in Table\\xa0 3. A \\nnumber of the experiments were carried out to determine the best value of batch size and \\nlearning rate, which are found to be 10 and 0.001, respectively.\\n3.3  Tweet text classification (long‑short‑term‑memory)\\nIn this work, the tweet is classified using the long-short-term-memory (LSTM) network. \\nLSTM is designed to remember important information for a longer period of time (Hochre-\\niter and Schmidhuber 1997; Sundermeyer et\\xa0al. 2012). As shown in Fig.\\xa0 3, the tweet texts \\nare passed through an embedding layer to get an embedded matrix. This tweet matrix is fed \\nto LSTM layers one after the other. In the first LSTM layer, 30 LSTM units are used, each \\nhaving 512-dimensional output space. Similarly, in the second LSTM layer, 30 LSTM units \\nare used, each having 256-dimensional output space. The output coming from the second \\nLSTM layer is then connected to 2 neurons, one for informative and other for not-inform-\\native classes. The categorical cross-entropy with the softmax activation function is used \\nwith Adam optimizer. The model is tested by varying the learning rate and batch size; the \\nbest performance was achieved with the learning rate of 0.001 and the batch size of 32. The \\ndetailed hyper-parameter settings for the model are listed in Table\\xa03. The detail description \\n(1)Softmax function= ̂yi =ϕ �xi\\n� = exi\\n∑M\\nk=1 exk\\n, where k = 1, 2,…,M , and xi ∈ R\\n(2)Categorical cross entropy=−\\nN/uni2211.s1\\ni=1\\nyilog/parenleft.s1̂yi\\n/parenright.s1\\nTable 3  Hyper-parameter settings for the proposed model\\nLSTM (Tweet text) VGG-16 (Image) Multi-modal (Tweet \\ntext + Image)\\nLoss function Categorical cross entropy Categorical cross entropy Categorical cross entropy\\nOptimizer Adam Adam Adam\\nLearning rate 0.001 0.001 0.001\\nEpochs 200 200 200\\nBatch size 32 10 10\\nActivation function tanh, Softmax ReLU, Softmax ReLU, tanh, Softmax'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 11, 'page_label': '802'}, page_content='802 Annals of Operations Research (2022) 319:791–822\\n1 3\\nregarding the creation of the tweet matrix and internal architecture of the LSTM unit can \\nbe seen in the subsequent sections.\\n3.3.1  Tweet text matrix representation\\nThe word embedding of the tweet is used to feed input to the model. The word embedding \\nrepresents each word of the corpus into a predefined fixed size real-valued vector. It creates \\na similar vector for words having similar meanings. The pre-trained word vector GloVe \\n(Global Vectors for word representation) (Pennington et\\xa0al. 2014) is used as the look-up \\nmatrix for this experiment. In our case, 100-dimensional GloVe word vector embedding \\n(glove.twitter.27B.100d.txt)2 is used, which is trained by Google on 27 billion words of \\ntweets. The advantage of using GloVe is, it reduces the computational overhead of the \\nmodel. Tweet matrix \\n/parenleft.s1T i\\n/parenright.s1 can be represented as:\\nwhere w1 ,w2 ,w3 ,…,wm represents the number of words in a tweet, and \\nem1 ,em2 ,em3 ,…,emK  represents the embedding of the word Wm . This tweet matrix has \\n(K × m) dimension, where K is the dimension of the embedding vector and m is the number \\nof words in the tweet. In this work, we fixed the total number of words for a tweet to 30, as \\nmost of the tweets contain 30 or less than 30 words. Padding is used where it is required to \\nmake all the tweets into the same length. As 100-dimensional GloVe embedding is used, so \\nin our case, the dimension of a tweet matrix is (30 × 100) , which is represented in Fig.\\xa0 3. \\nThis tweet matrix is then used by LSTM layers to learn the salient features from them. As \\nthe tweet matrix is represented in (30 × 100) dimension, 30 LSTM units are used to pro-\\ncess the embedding of each word. The detail description of an LSTM unit can be seen in \\nSect.\\xa03.3.2.\\n3.3.2  Long‑short term memory (LSTM) unit\\nThis section discusses the detail working principle of an LSTM unit. Each LSTM unit con-\\ntains four components: (i) forget gate /parenleft.s1ft\\n/parenright.s1 , (ii) input gate /parenleft.s1it\\n/parenright.s1 , (iii) cell state /parenleft.s1C t\\n/parenright.s1 , and (iv) \\noutput gate /parenleft.s1O t\\n/parenright.s1 . The cell state keeps the relevant information throughout the processing \\nof the sequences. This cell state can be considered as the “memory” of the network. The \\ninformation is added or deleted using gates throughout the journey of the cell state. The \\nforget gate decides which information should be kept or thrown away based on their impor-\\ntance. Input gate is used to update the cell state, and the output gate decides what will be \\nthe next hidden state. During training the model, these gates learn which information in a \\nsequence is important to keep or forget. They pass only those information to the cell state, \\nwhich is important for the prediction. The detailed internal architecture of an LSTM unit \\nis shown in Fig.\\xa0 4. In the figure, C t−1 and C t represent cell state for time step t − 1 and t, \\nW 1 W 2 W 3 ⋯ W m\\nTi =\\n⎡\\n⎢\\n⎢\\n⎢\\n⎢⎣\\ne11 e21 e31 ⋯ em1\\ne12 e22 e32 … em2\\ne13 e23 e33 … em3\\n⋮ ……… ⋮\\ne1K e2K e3K ⋯ emK\\n⎤\\n⎥\\n⎥\\n⎥\\n⎥⎦\\n2 It is freely available at https ://nlp.stanf ord.edu/proje cts/glove /.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 12, 'page_label': '803'}, page_content='803Annals of Operations Research (2022) 319:791–822 \\n1 3\\nrespectively. Similarly, h t−1 and h t represents hidden layer output at time step t − 1 and t, \\nrespectively. The input feature to the LSTM unit is denoted by X t . It contains sigmoid and \\ntanh activation function which can be defined by Eqs.\\xa0(3) and (4), respectively.\\nThe value of sigmoid and tanh activation function ranges from 0 to 1 and − 1 to 1 respec-\\ntively. These values are basically responsible for all the gate operation. The forget gate \\n/parenleft.s1ft\\n/parenright.s1 , \\ninput gate /parenleft.s1it\\n/parenright.s1 , cell state /parenleft.s1C t\\n/parenright.s1 , and output gate /parenleft.s1O t\\n/parenright.s1 mathematically can be represented by \\nEqs.\\xa0(5), (6), (7), and (8), respectively.\\nwhere /u1D6FCf, /u1D6FCi, /u1D6FCC , /u1D6FCO  are the weight matrices and /u1D6FDf, /u1D6FDi, /u1D6FDC , /u1D6FDO  are the bias values for the for-\\nget gate, input gate, cell state, and output gate, respectively. Finally, the hidden layer output \\nat time step t  can be defined as ht = O t × tanh/parenleft.s1Ct\\n/parenright.s1 . This hidden layer output is then con-\\nnected with the next LSTM unit.\\n3.4  Multi‑modal system (VGG‑16 + LSTM)\\nThe proposed multi-modal system uses both tweet texts and images for the classification of \\ninformative and not-informative content. The second-last layer of the VGG-16 model con-\\ntaining 4096 neurons in their dense layer is mapped to another dense layer containing 256 \\n(3)Sigmoid function ∶ /u1D70E/parenleft.s1xi\\n/parenright.s1= 1\\n1 + e−xi\\n, where xi ∈ R\\n(4)tanh /parenleft.s1xi\\n/parenright.s1= exi − e−xi\\nexi + e−xi\\n, where xi ∈ R\\n(5)Forget gate /parenleft.s1ft\\n/parenright.s1= /u1D70E/parenleft.s1/u1D6FCf ./bracketleft.s1h t−1\\n/bracketright.s1+ /u1D6FDf\\n/parenright.s1\\n(6)Input gate/parenleft.s1it\\n/parenright.s1= /u1D70E/parenleft.s1/u1D6FCi ⋅ /bracketleft.s1h t−1 ,xt\\n/bracketright.s1+ /u1D6FDi\\n/parenright.s1\\n(7)Cell state/parenleft.s1C t\\n/parenright.s1= ft × C t−1 + it × C �\\nt\\nC �\\nt = tanh/parenleft.s1/u1D6FCC ⋅ /bracketleft.s1ht−1 ,xt\\n/bracketright.s1+ /u1D6FDC\\n/parenright.s1\\n(8)Output gate /parenleft.s1O t\\n/parenright.s1= /u1D70E/parenleft.s1/u1D6FCO ⋅ /bracketleft.s1ht−1 ,xt\\n/bracketright.s1+ /u1D6FDO\\n/parenright.s1\\nFig. 4  The detailed internal \\narchitecture of a LSTM unit'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 13, 'page_label': '804'}, page_content='804 Annals of Operations Research (2022) 319:791–822\\n1 3\\nneurons. The last layer of the LSTM model is then concatenated with the 256-dimensional \\nfeature map of the VGG-16 model. Total of 512-dimensional feature map is generated by \\nconcatenating both the layers as can be seen from Fig.\\xa0 3. Finally, concatenated feature \\nmaps are then mapped to the output layer containing two neurons, one for the informative \\nand one for the not-informative classes. The label of tweet text is used as the final label \\nfor the concatenated features of tweet texts and images. As in the case of the VGG-16 \\nmodel, the weights are marked as non-trainable up to the second last layer, so here, the \\nsame procedure is applied. Similarly, softmax activation function at the output layer, cat-\\negorical cross-entropy as a loss function, Adam, as the optimizer with a learning rate of \\n0.001, is used. The model performed best with the learning rate and batch size of 0.001 and \\n10, respectively.\\n3.5  Majority voting\\nIn the majority voting strategy, all the three models LSTM (Tweet text), VGG-16 (Image), \\nand Multi-modal (Tweet text + Image) are used. The prediction of each of the models is \\nconsidered, and the final prediction is assigned based on the majority. If at least two mod-\\nels predicted a class, then the final label is assigned with that class label. The finding of \\nthis strategy is discussed in detail in Sect.\\xa04.\\n4  Results\\nThe extensive experiments have been done to validate the proposed model under two cate-\\ngories: (i) event specific experiment: the system was trained and tested with the same event \\ndata only and (ii) cross-event experiment: the system was trained with a specific event \\nbut tested with other event data. The performance of the system has been evaluated using \\nprecision, recall, and F1-score. The description of the used evaluation metrics is given in \\nSect.\\xa04.1.\\n4.1  Evaluation metrics\\n• Precision: Precision for a class (say informative class) can be defined as, number of \\naccurately predicted informative contents to the total number of predicted informative \\ncontents. The value of precision varies between 0 and 1, where 0 means the worst per -\\nformance and 1 means the best performance.\\n• Recall: Recall for a class (say informative class) can be defined as, number of accu-\\nrately predicted informative contents to the total number of informative contents in the \\ndataset. The value of recall varies from 0 to 1, where 0 is the worst performance and 1 \\nis the best performance.\\nPrecision= Number of accurately predicted informative contents\\nTotal number of predicted informative contents\\nRecall= Number of accurately predicted informative contents\\nTotal number of actual informative contents'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 14, 'page_label': '805'}, page_content='805Annals of Operations Research (2022) 319:791–822 \\n1 3\\n• F1-score: F1-score is the harmonic mean between precision and recall. The value of \\nF1-score varies from 0 to 1, where 0 indicates the worst performance, whereas 1 indi-\\ncates the best performance.\\nThe weighted average of precision, recall, and F1-score of informative and not-informa-\\ntive classes are reported to evaluate the performance of the proposed model. In our experi-\\nment, all the three models LSTM (Tweet text), VGG-16 (Image), and Multi-modal (Tweet \\ntext + Image) are trained separately with the dataset of (i) Hurricane Harvey, (ii) Hurricane \\nIrma, (iii) Hurricane Maria, (iv) Iran–Iraq earthquake, (v) Mexico earthquake, (vi) Cali-\\nfornia wildfires, and (vii) Sri Lanka flood. Then, all the trained models are tested individu-\\nally with all the possible combinations of testing data. For example, if the LSTM (Tweet \\ntext) is trained with say Hurricane Harvey, it is tested with all seven datasets. Similarly, \\nwhen LSTM (Tweet text) is trained with Hurricane Irma, it is tested with all seven datasets. \\nLikewise, for one model, say LSTM (Tweet text), we performed 49 testing experiments. \\nWe have three models, LSTM (Tweet text), VGG-16 (Image), and Multi-modal, and for \\neach, we formed 49 test cases. Therefore, in total, 147 test cases were formed to evaluate \\nthe performance of the models. The results for the models when trained with (i) Hurricane \\nHarvey, (ii) Hurricane Irma, (iii) Hurricane Maria, (iv) Iran–Iraq earthquake, (v) Mexico \\nearthquake, (vi) California wildfires, and (vii) Sri Lanka flood separately and tested with \\nall the possible combinations of the testing data are shown in Tables\\xa04, 5, 6, 7, 8, 9, and 10 \\nrespectively. The confusion matrices of the best performing model when tested with the \\nsame event data for (i) Hurricane Harvey, (ii) Hurricane Irma, (iii) Hurricane Maria, (iv) \\nIran–Iraq earthquake, (v) Mexico earthquake, (vi) California wildfires, and (vii) Sri Lanka \\nflood are shown in Figs.\\xa05, 6, 7, 8, 9, 10, and 11, respectively.\\n5  Discussion\\nOur findings suggest that the identification of informative Twitter contents related to disas-\\nter using images and text together with a majority voting scheme is better than the models \\nutilizing text alone. It is also found that in the case of earthquakes and wildfires event, \\nimages alone are performing better than text in identifying informative contents but the \\nauthenticity of images are questionable as people post old images of similar events dur -\\ning the current disaster event (Gupta et\\xa0 al. 2013; Imran et\\xa0 al. 2015). Therefore, for the \\nexperimentation, we have considered the label of text as the final label for the multi-modal \\nsettings. The finding of this research also suggests that the embedding of images with the \\ntweet text performed significantly better in identifying disaster-related informative con-\\ntents. The proposed systems are validated for two different settings: (i) In-event validation \\nand (ii) Cross-event validation. In the case of In-event validation, the system is trained and \\ntested with the same event dataset, whereas in the case of Cross-event validation, the sys-\\ntem is trained with one dataset and tested with different event datasets. In both the In-event \\nand Cross-event settings, out of the total 49 sets of testing combinations, the model with \\nthe Majority voting scheme outperformed the LSTM model in 39 cases where only tweet \\ntexts have been used. In the remaining 10 cases, the Majority voting scheme gives compa-\\nrable results with respect to the LSTM model.\\nF1 − score= 2 × Precision× Recall\\nPrecision+ Recall'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 15, 'page_label': '806'}, page_content='806 Annals of Operations Research (2022) 319:791–822\\n1 3\\nTable 4  Results of various models when it is trained with Hurricane Harvey dataset\\nHurricane Harvey\\nModels Hurricane Harvey Hurricane Maria Hurricane Irma California Wildfires Mexico Earthquake Iraq–Iran Earth-\\nquake\\nSri Lanka Flood\\nP R F1 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1\\nLSTM (Tweet Text) 0.81 0.81 0.81 0.74 0.72 0.72 0.74 0.70 0.69 0.56 0.56 0.56 0.65 0.65 0.65 0.52 0.52 0.52 0.84 0.80 0.80\\nVGG-16 (Image) 0.81 0.81 0.81 0.70 0.71 0.70 0.74 0.74 0.74 0.75 0.75 0.75 0.82 0.81 0.81 0.84 0.83 0.83 0.79 0.76 0.77\\nMulti-modal 0.83 0.82 0.82 0.76 0.76 0.76 0.75 0.75 0.75 0.63 0.62 0.62 0.72 0.66 0.65 0.66 0.62 0.60 0.87 0.87 0.87\\nMajority voting 0.84 0.84 0.84 0.76 0.76 0.76 0.76 0.75 0.75 0.63 0.63 0.63 0.70 0.69 0.69 0.60 0.60 0.59 0.87 0.85 0.85'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 16, 'page_label': '807'}, page_content='807Annals of Operations Research (2022) 319:791–822 \\n1 3\\nTable 5  Results of various models when it is trained with Hurricane Irma dataset\\nHurricane Irma\\nModels Hurricane Irma Hurricane Harvey Hurricane Maria Sri Lanka flood Mexico Earthquake Iraq–Iran Earth-\\nquake\\nCalifornia Wildfires\\nP R F1 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1\\nLSTM (Tweet Text) 0.83 0.83 0.83 0.74 0.72 0.71 0.76 0.76 0.75 0.84 0.81 0.81 0.69 0.63 0.62 0.71 0.64 0.62 0.68 0.65 0.65\\nVGG-16 (Image) 0.83 0.83 0.83 0.74 0.74 0.74 0.76 0.76 0.76 0.76 0.76 0.76 0.83 0.83 0.83 0.74 0.71 0.72 0.72 0.71 0.70\\nMulti-modal 0.81 0.81 0.81 0.76 0.75 0.75 0.78 0.78 0.78 0.83 0.78 0.77 0.64 0.61 0.60 0.68 0.64 0.63 0.66 0.66 0.66\\nMajority voting 0.82 0.82 0.82 0.77 0.75 0.74 0.79 0.79 0.79 0.82 0.78 0.78 0.71 0.67 0.67 0.74 0.69 0.68 0.70 0.70 0.70'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 17, 'page_label': '808'}, page_content='808 Annals of Operations Research (2022) 319:791–822\\n1 3\\nTable 6  Results of various models when it is trained with Hurricane Maria dataset\\nHurricane Maria\\nModels Hurricane Maria Hurricane Harvey Hurricane Irma Sri Lanka flood Mexico Earthquake Iraq–Iran Earth-\\nquake\\nCalifornia Wildfires\\nP R F1 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1\\nLSTM (Tweet Text) 0.83 0.83 0.82 0.72 0.72 0.72 0.77 0.76 0.76 0.90 0.90 0.90 0.68 0.57 0.53 0.77 0.57 0.49 0.61 0.60 0.60\\nVGG-16 (Image) 0.78 0.78 0.78 0.73 0.73 0.73 0.75 0.75 0.75 0.81 0.81 0.81 0.79 0.79 0.79 0.82 0.81 0.81 0.67 0.65 0.63\\nMulti-modal 0.83 0.83 0.83 0.77 0.76 0.76 0.78 0.72 0.71 0.90 0.90 0.90 0.66 0.58 0.55 0.77 0.55 0.45 0.60 0.59 0.59\\nMajority voting 0.84 0.84 0.84 0.77 0.77 0.77 0.78 0.76 0.76 0.91 0.91 0.91 0.66 0.58 0.56 0.77 0.57 0.49 0.63 0.62 0.62'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 18, 'page_label': '809'}, page_content='809Annals of Operations Research (2022) 319:791–822 \\n1 3\\nTable 7  Results of various models when it is trained with Iraq–Iran earthquake dataset\\nIraq–Iran Earthquake\\nModels Iraq–Iran Earth-\\nquake\\nMexico Earthquake Hurricane Harvey Hurricane Maria Hurricane Irma California Wildfires Sri Lanka flood\\nP R F1 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1\\nLSTM (Tweet Text) 0.81 0.81 0.81 0.66 0.67 0.66 0.59 0.51 0.35 0.75 0.50 0.34 0.62 0.50 0.34 0.75 0.55 0.40 0.76 0.44 0.28\\nVGG-16 (Image) 0.86 0.86 0.85 0.81 0.79 0.78 0.60 0.59 0.51 0.67 0.60 0.53 0.58 0.59 0.51 0.47 0.51 0.38 0.71 0.71 0.68\\nMulti-modal 0.79 0.79 0.79 0.74 0.74 0.74 0.65 0.62 0.59 0.59 0.57 0.53 0.58 0.56 0.54 0.56 0.56 0.48 0.70 0.60 0.58\\nMajority voting 0.84 0.83 0.83 0.75 0.75 0.75 0.68 0.55 0.44 0.56 0.51 0.41 0.54 0.51 0.40 0.66 0.56 0.42 0.78 0.52 0.44'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 19, 'page_label': '810'}, page_content='810 Annals of Operations Research (2022) 319:791–822\\n1 3\\nTable 8  Results of various models when it is trained with Mexico earthquake dataset\\nMexico earthquake\\nModels Mexico Earthquake Iraq–Iran Earth-\\nquake\\nHurricane Harvey Hurricane Maria Hurricane Irma Sri Lanka flood California Wildfires\\nP R F1 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1\\nLSTM (Tweet Text) 0.71 0.71 0.71 0.63 0.60 0.58 0.72 0.66 0.64 0.60 0.56 0.52 0.64 0.55 0.48 0.74 0.57 0.53 0.53 0.54 0.52\\nVGG-16 (Image) 0.87 0.87 0.87 0.78 0.79 0.78 0.66 0.66 0.66 0.66 0.66 0.65 0.68 0.68 0.67 0.81 0.81 0.81 0.73 0.72 0.71\\nMulti-modal 0.73 0.72 0.72 0.80 0.67 0.62 0.66 0.58 0.51 0.60 0.55 0.48 0.62 0.56 0.50 0.65 0.55 0.51 0.63 0.61 0.58\\nMajority voting 0.75 0.74 0.74 0.71 0.67 0.64 0.70 0.64 0.60 0.61 0.56 0.51 0.65 0.58 0.53 0.69 0.58 0.55 0.63 0.62 0.59'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 20, 'page_label': '811'}, page_content='811Annals of Operations Research (2022) 319:791–822 \\n1 3\\nTable 9  Results of various models when it is trained with California wildfires dataset\\nCalifornia wildfires\\nModels California Wildfires Hurricane Harvey Hurricane Irma Hurricane Maria Mexico Earthquake Iraq–Iran Earth-\\nquake\\nSri Lanka flood\\nP R F1 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1\\nLSTM (Tweet Text) 0.64 0.64 0.64 0.45 0.47 0.41 0.51 0.50 0.45 0.50 0.50 0.46 0.42 0.47 0.42 0.65 0.64 0.63 0.43 0.41 0.39\\nVGG-16 (Image) 0.83 0.82 0.82 0.71 0.71 0.70 0.73 0.74 0.73 0.69 0.69 0.68 0.76 0.76 0.76 0.73 0.74 0.74 0.79 0.79 0.79\\nMulti-modal 0.74 0.73 0.73 0.64 0.60 0.57 0.61 0.59 0.57 0.59 0.57 0.55 0.65 0.63 0.60 0.61 0.57 0.51 0.66 0.57 0.55\\nMajority voting 0.75 0.74 0.74 0.63 0.60 0.58 0.60 0.59 0.57 0.62 0.59 0.57 0.66 0.65 0.62 0.62 0.60 0.56 0.66 0.58 0.56'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 21, 'page_label': '812'}, page_content='812 Annals of Operations Research (2022) 319:791–822\\n1 3\\nTable 10  Results of various models when it is trained with Sri Lanka flood dataset\\nSri Lanka flood\\nModels Sri Lanka flood Hurricane Harvey Hurricane Maria Hurricane Irma Mexico Earthquake Iraq–Iran Earth-\\nquake\\nCalifornia Wildfires\\nP R F1 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1\\nLSTM (Tweet Text) 0.92 0.92 0.92 0.73 0.70 0.69 0.74 0.69 0.68 0.76 0.72 0.71 0.61 0.62 0.60 0.70 0.67 0.66 0.58 0.57 0.53\\nVGG-16 (Image) 0.87 0.85 0.84 0.67 0.61 0.53 0.71 0.62 0.56 0.67 0.64 0.58 0.81 0.80 0.79 0.79 0.79 0.76 0.66 0.56 0.45\\nMulti-modal 0.94 0.94 0.94 0.74 0.74 0.74 0.74 0.74 0.74 0.80 0.80 0.80 0.57 0.57 0.57 0.77 0.69 0.67 0.54 0.54 0.53\\nMajority voting 0.93 0.93 0.93 0.75 0.72 0.71 0.74 0.70 0.68 0.75 0.72 0.71 0.63 0.63 0.62 0.72 0.69 0.68 0.58 0.57 0.53'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 22, 'page_label': '813'}, page_content='813Annals of Operations Research (2022) 319:791–822 \\n1 3\\nFig. 5  Confusion matrix when \\nsystem was trained and tested \\nwith Hurricane Harvey dataset\\nFig. 6  Confusion matrix when \\nsystem was trained and tested \\nwith Hurricane Irma dataset\\nFig. 7  Confusion matrix when \\nsystem was trained and tested \\nwith Hurricane Maria dataset'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 23, 'page_label': '814'}, page_content='814 Annals of Operations Research (2022) 319:791–822\\n1 3\\nFig. 8  Confusion matrix when \\nsystem was trained and tested \\nwith Iraq–Iran earthquake dataset\\nFig. 9  Confusion matrix when \\nsystem was trained and tested \\nwith Mexico earthquake dataset\\nFig. 10  Confusion matrix when \\nsystem was trained and tested \\nwith California wildfire dataset'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 24, 'page_label': '815'}, page_content='815Annals of Operations Research (2022) 319:791–822 \\n1 3\\nIn-event validation: (a) Hurricane Harvey: The Majority voting scheme and the multi-\\nmodal system performed better than the LSTM model where only tweet text was used. The \\nMajority voting scheme achieved an F1-score of 0.84, whereas the LSTM model achieved \\nan F1-score of 0.81 in classifying informative and not-informative Twitter contents. The \\nconfusion matrix shown in Fig.\\xa0 5 for the Majority voting system when it is trained and \\ntested with Hurricane Harvey shows that out of 100 informative contents, the model pre-\\ndicts 84 contents as the informative. (b) Hurricane Irma: The best result was obtained for \\nLSTM model with tweet text (F1-score = 0.83) but the majority voting scheme has also \\ngiven comparable results (F1-score = 0.82). (c) Hurricane Maria: The Majority voting and \\nmulti-modal system performed better than the LSTM model, where only the tweet text \\nwas used. The Majority voting and Multi-modal system achieved an F1-score of 0.84 and \\n0.83, respectively, whereas the LSTM model achieved an F1-score of 0.82. (d) Iraq–Iran \\nearthquake: The Majority voting system achieved an F1-score of 0.83, which is better than \\nthe LSTM model as it achieved an F1-score of 0.81. (e) Mexico earthquake: The Major -\\nity voting achieved an F1-score of 0.74, which is 3% higher than the LSTM model where \\nonly tweet text was used. (f) California Wildfire: In this case, the Majority voting system \\nperformed better than the LSTM model with a margin of 10% in the F1-score. The Major -\\nity voting system achieved an F1-score of 0.74, whereas the LSTM model achieved an \\nF1-score of 0.64. (g) Sri Lanka floods: A similar kind of result is also found in the case of \\nSri Lanka floods. The Majority voting scheme achieved an F1-score of 0.93, which is 1% \\nhigher than the LSTM model. In the case of In-event validation, out of the total seven dif-\\nferent event dataset, the combination of text and images both for Majority voting system \\nperformed better than the LSTM model for six events, namely, Hurricane Harvey, Hurri-\\ncane Maria, Iraq–Iran earthquake, Mexico earthquake, California wildfires, and Sri Lanka \\nfloods. In the case of Hurricane Irma, the Majority voting system achieved a comparable \\nresult to the LSTM model.\\nCross-event validation: (a) Hurricane Harvey: When the models are tested with Hur -\\nricane Irma and Hurricane Maria, the Majority voting and Multi-modal system performed \\nbetter than the LSTM model. As the nature of the Hurricane Irma and Hurricane Maria \\nevents are same as the Hurricane Harvey, the proposed model performed significantly well. \\nThe Majority voting system achieved an F1-score of 0.76 and 0.75 for Hurricane Maria \\nand Hurricane Irma events, respectively. When the models are tested with the cross-event \\nFig. 11  Confusion matrix when \\nsystem was trained and tested \\nwith Sri Lanka flood dataset'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 25, 'page_label': '816'}, page_content='816 Annals of Operations Research (2022) 319:791–822\\n1 3\\ndataset (California wildfires, Mexico earthquake, and Iraq–Iran earthquake), the perfor -\\nmance of the model has been degraded as the nature of the event is changed, although the \\nMajority voting scheme gives a better result in comparison to LSTM. While testing the \\nmodel with the Sri Lanka flood dataset, the model performed well with an F1-score of \\n0.85, because the text and images in both the cases contained water as a component. One \\nof the examples for Hurricane Harvey is: “RT NickABC13: Yes, that’s a Cadillac stuck in \\nwater. The driver had to be rescued. #Harvey https ://t.co/c3c8l v0MQo ”, and one example \\nfor the Sri Lanka flood is: “Mora Impact: Port city sees heavy rain, waterlogging #TIS-\\nNews Click Link- https ://t.co/mn9pv B4s1t  https ://t.co/6ywgX iVJEO ”. In both the tweets, \\npeople are talking about water, which is one of the possible reasons why these types of \\ncross-event testing perform better. (b) Hurricane Irma: A similar kind of result is found in \\nthe case of Hurricane Irma as well. The Majority voting system performed significantly \\nbetter than the LSTM model in the case of Hurricane Harvey, Hurricane Maria, Mexico \\nearthquake, Iran–Iraq earthquake, and California wildfires events. But in the case of Sri \\nLanka flood, the F1-score of Majority voting was 0.78, which is less than the LSTM model \\n(F1-score = 0.81). The performance of the multi-modal system is also slightly degraded \\nin comparison to the LSTM model. (c) Hurricane Maria: When the model is trained with \\nHurricane Maria, the Majority voting system performed better than the LSTM model \\nthroughout all the cross-event testing (see Table\\xa06).\\n(d) Iraq–Iran earthquake: The cross-event testing with the Majority voting system per -\\nformed significantly better than the LSTM model throughout all the testing events (see \\nTable\\xa07). The Mexico earthquake has the same nature as the Iran–Iraq earthquake. The \\nperformance of the Majority voting scheme is better in comparison to the LSTM model \\nby a margin of 9% (F1-score = 0.66 in case of only text, whereas F1-score of 0.75 in case \\nof the Majority voting scheme). (e) Mexico earthquake: Similarly, the system trained with \\nthe Mexico earthquake, the Majority scheme performed better than the LSTM model in \\ncase of Iraq–Iran earthquake, Hurricane Irma, Sri Lanka flood, and California wildfire (see \\nTable\\xa08). (f) California wildfires: The cross-event testing with the Majority voting system \\nperformed significantly well for Hurricane Harvey, Hurricane Irma, Hurricane Maria, \\nMexico earthquake, and Sri Lanka floods. In the case of the Iraq–Iran earthquake event, \\nthe performance of the Majority voting system degraded slightly. (g) Sri Lanka floods: The \\nMajority voting system performed better than the LSTM model throughout all the testing \\ncases (see Table\\xa0 10). Similarly, the model performed well when it was tested with Hurri-\\ncane Harvey, Hurricane Irma, and Hurricane Maria in comparison with other cross-event \\ntesting cases. The possible reason for this is similar to the case of Hurricane as it contains \\ntweets related to water-logging and area in the image filled with water.\\nOur results are better than recently proposed similar works by Nguyen et\\xa0 al. (2016, \\n2017a) and Caragea et\\xa0al. (2016). Nguyen et\\xa0al. (2016, 2017a) used Convolutional Neural \\nNetwork (CNN), whereas Caragea et\\xa0al. (2016) used Support Vector Machine (SVM), Ran-\\ndom Forest, Logistic Regression, and (CNN) techniques. In order to compare our models \\nwith them, we tested these models with the datasets we have used. For SVM, Random \\nForest, and Logistic Regression unigram, bigram, and trigram TF-IDF features were used. \\nFor CNN, 2-gram, 3-gram, and 4-gram filters were used to extract features from the tweet \\ntexts. The result of each of the models is shown in Table\\xa0 11. The proposed Majority vot-\\ning scheme outperformed all the existing works across all the datasets except Hurricane \\nIrma. Even LSTM model with tweet text only also outperformed all the text classifica-\\ntion techniques such as SVM, Random Forest, Logistic Regression, and Convolutional \\nNeural Network (CNN). One of the limitations of this work is that we have not checked \\nthe authenticity of images used for the experimentation. It has been observed that several'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 26, 'page_label': '817'}, page_content='817Annals of Operations Research (2022) 319:791–822 \\n1 3\\nTable 11  Comparison of the proposed work with the existing methodologies\\nHurricane Harvey \\nF1-score\\nHurricane Maria \\nF1-score\\nHurricane Irma \\nF1-score\\nIraq–Iran earth-\\nquake F1-score\\nMexico earth-\\nquake F1-score\\nSri Lanka Flood \\nF1-score\\nCalifornia \\nWildfire \\nF1-score\\nSVM (Text) 0.32 0.33 0.33 0.31 0.27 0.26 0.29\\nRandom Forest (Text) 0.75 0.81 0.72 0.69 0.68 0.83 0.60\\nLogistic Regression (Text) 0.78 0.82 0.80 0.74 0.72 0.82 0.60\\nCNN (Text) 0.81 0.82 0.80 0.74 0.68 0.92 0.61\\nLSTM (Text) 0.81 0.82 0.83 0.81 0.71 0.92 0.64\\nMajority voting (Image + Text) 0.84 0.84 0.82 0.83 0.74 0.93 0.74'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 27, 'page_label': '818'}, page_content='818 Annals of Operations Research (2022) 319:791–822\\n1 3\\nold and similar images are posted by the users during disaster. Therefore, a system can be \\ndeveloped to filter these old and similar images.\\n5.1  Theoretical contributions\\nOne of the major theoretical contributions of this research is the development of a par -\\nallel system with LSTM for tweet text and VGG-16 for images of disastrous scenarios \\nto classify informative and not-informative Twitter contents. The proposed system does \\nnot require any human efforts to extract features for training the model. The other theo-\\nretical contribution is that the proposed system uses a pre-trained VGG-16 network, which \\nreduces the overall training time of the system as it is required in the case of disaster.\\nThe proposed system is validated with both In-event and Cross-event disasters, and it \\nsignificantly performed better than the systems where only tweet text was used. Therefore, \\nthis system can be better utilized in the situation of cross-event disaster in the early stage \\nwhere a less number of the disaster-specific labeled data is available. The proposed system \\ncan be utilized in all the three types of input data, such as only tweet text, only images, \\nand tweet text and images together to classify informative and non-informative Twitter \\ncontents.\\n5.2  Implications for\\xa0practice\\nThe model can be implemented in any system to segregate the informative tweets from \\nnon-informative tweets using either text, images, or both together. These informative tweets \\ncan then be used by humanitarian organizations to know the floor reality of the disaster. \\nThis system can be integrated with any social media platform to filter informative contents \\nform massive social media content. An android application can also be made where this \\nsystem can separate disaster-related informative contents from the live streaming of social \\nmedia posts to help people become more situationally aware in the case of disaster. This \\nmulti-modal system can be utilized in finding relevant information in other domains also \\nsuch as finding relevant content related to road accidents and civil unrest if domain-specific \\ntraining is done.\\n6  Conclusion\\nThe identification of disaster-related informative messages from Twitter is a challeng-\\ning task as tweets have several grammatical mistakes, non-standard abbreviations, and \\nlimited word space. In this work, a multi-modal system is proposed which utilizes tweet \\ntexts as well as images to identify informative Twitter contents. The system uses LSTM \\nand VGG-16 for tweet text and image, respectively. We have used seven different dis-\\nasters related Twitter datasets and achieved an F1-score of 0.84, 0.84, 0.82, 0.83, 0.74, \\n0.93, and 0.74 for Hurricane Harvey, Hurricane Maria, Hurricane Irma, Iraq–Iran earth-\\nquake, Mexico earthquake, Sri Lanka flood, and California wildfires, respectively, in \\nthe case of Majority voting scheme. These results have outperformed the other models \\nwhere only tweet text is used. This system can also be utilized in other similar kinds of \\ncrisis events, as in our case, we have tested it with Hurricane and flood that has achieved \\nsignificant results. This model can be used for the primary filtration of informative \\ntweets from the massive amount of tweets. Then, the informative tweets can be further'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 28, 'page_label': '819'}, page_content='819Annals of Operations Research (2022) 319:791–822 \\n1 3\\nclassified into several classes such as infrastructure and utility damage, affected indi-\\nviduals, injured or dead people, and vehicle damage for providing better rescue and \\nrelief operation. The limitation of this work is that we have considered English language \\ntweets only, but during an emergency, people also post their tweets in regional lan-\\nguages. So, a deep neural network-based model can be developed to deal with the issues \\nof multi-linguality. As the F1-score of the proposed approaches varies in the range of \\n0.74 to 0.93, the system can be enhanced to achieve better accuracy.\\nReferences\\nAipe, A., Ekbal, A., Mukuntha, N., & Kurohashi, S. (2018). Linguistic feature assisted deep learning \\napproach towards multi-label classification of crisis related tweets. In Proceedings of the 15th \\nISCRAM Conference (pp. 705–717).\\nAkter, S., & Wamba, S. F. (2017). Big data and disaster management: A systematic review and agenda \\nfor future research. Annals of Operations Research. https ://doi.org/10.1007/s1047 9-017-2584-2.\\nAlalwan, A. A., Rana, N. P., Dwivedi, Y. K., & Algharabat, R. (2017). Social media in marketing: A \\nreview and analysis of the existing literature. Telematics and Informatics, 34(7), 1177–1190.\\nAlam, F., Imran, M., & Ofli, F. (2017). Image4act: Online social media image processing for disaster \\nresponse. In Proceedings of the 2017 IEEE/ACM international conference on advances in social \\nnetworks analysis and mining 2017 (pp. 601–604). ACM.\\nAlam, F., Ofli, F., & Imran, M. (2018). Crisismmd: Multimodal Twitter datasets from natural disasters. \\nIn Proceedings of the 12th international AAAI conference on web and social media (ICWSM).\\nAshktorab, Z., Brown, C., Nandi, M., & Culotta, A. (2014). Tweedr: Mining Twitter to inform disaster \\nresponse. In Proceedings of the 11th ISCRAM conference (pp. 354–358).\\nAtefeh, F., & Khreich, W. (2015). A survey of techniques for event detection in Twitter. Computational \\nIntelligence, 31, 132–164.\\nCameron, M. A., Power, R., Robinson, B., & Yin, J. (2012). Emergency situation awareness from Twit-\\nter for crisis management. In Proceedings of the 21st international conference on world wide web  \\n(pp. 695–698). ACM.\\nCaragea, C., Mcneese, N., Jaiswal, A., Traylor, G., Woo Kim, H., Mitra, P., et\\xa0al. (2011). Classifying text \\nmessages for the Haiti earthquake. In Proceedings of the 8th international conference on informa-\\ntion systems for crisis response and management (ISCRAM2011).\\nCaragea, C., Silvescu, A., & Tapia, A. H. (2016). Identifying informative messages in disaster events \\nusing convolutional neural networks. In International conference on information systems for crisis \\nresponse and management (pp. 137–147).\\nChaudhuri, N., & Bose, I. (2019). Application of image analytics for disaster response in smart cit-\\nies. In Proceedings of the 52nd Hawaii international conference on system sciences. https ://doi.\\norg/10.24251 /hicss .2019.367\\nDaly, S., & Thom, J. A. (2016). Mining and classifying image posts on social media to analyse fires. In \\nProceedings of the 13th ISCRAM conference (pp. 1–14).\\nDubey, R. (2019). Developing an integration framework for crowdsourcing and internet of things with \\napplications for disaster response. In Social entrepreneurship: Concepts, methodologies, tools, and \\napplications (pp. 274–283). IGI Global.\\nDubey, R., Ali, S. S., Aital, P., Venkatesh, V., et\\xa0al. (2014). Mechanics of humanitarian supply chain \\nagility and resilience and its empirical validation. International Journal of Services and Operations \\nManagement, 17, 367–384.\\nDubey, R., Altay, N., & Blome, C. (2017). Swift trust and commitment: The missing links for humani-\\ntarian supply chain coordination? Annals of Operations Research. https ://doi.org/10.1007/s1047  \\n9-017-2676-z.\\nDubey, R., Gunasekaran, A., Childe, S. J., Roubaud, D., Wamba, S. F., Giannakis, M., et\\xa0 al. (2019). \\nBig data analytics and organizational culture as complements to swift trust and collaborative per -\\nformance in the humanitarian supply chain. International Journal of Production Economics, 210, \\n120–136.\\nDwivedi, Y. K., Kapoor, K. K., & Chen, H. (2015). Social media marketing and advertising. The Marketing \\nReview, 15(3), 289–309.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 29, 'page_label': '820'}, page_content='820 Annals of Operations Research (2022) 319:791–822\\n1 3\\nDwivedi, Y. K., Shareef, M. A., Mukerji, B., Rana, N. P., & Kapoor, K. K. (2018). Involvement in emer -\\ngency supply chain for disaster management: A cognitive dissonance perspective. International \\nJournal of Production Research, 56, 6758–6773.\\nElbanna, A., Bunker, D., Levine, L., & Sleigh, A. (2019). Emergency management in the changing world \\nof social media: Framing the research agenda with the stakeholders through engaged scholarship. \\nInternational Journal of Information Management, 47, 112–120.\\nGraf, D., Retschitzegger, W., Schwinger, W., Pröll, B., & Kapsammer, E. (2018). Cross-domain informa-\\ntiveness classification for disaster situations. In Proceedings of the 10th international conference on \\nmanagement of digital ecosystems (pp. 183–190). ACM.\\nGuha-Sapir, D., Vos, F., Below, R., & Ponserre, S. (2012). Annual disaster statistical review 2011: \\nThe number sandtrends. Technical Report Centre for Research on the Epidemiology of Disasters \\n(CRED).\\nGupta, A., Lamba, H., Kumaraguru, P., & Joshi, A. (2013, May). Faking sandy: Characterizing and iden-\\ntifying fake images on Twitter during hurricane sandy. In Proceedings of the 22nd international \\nconference on world wide web (pp. 729–736). ACM.\\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9, 1735–1780.\\nHuang, Q., & Xiao, Y. (2015). Geographic situational awareness: Mining tweets for disaster prepared-\\nness, emergency response, impact, and recovery. ISPRS International Journal of Geo-Information,  \\n4, 1549–1568.\\nImran, M., Castillo, C., Diaz, F., & Vieweg, S. (2015). Processing social media messages in mass emer -\\ngency: A survey. ACM Computing Surveys (CSUR), 47, 67.\\nImran, M., Castillo, C., Lucas, J., Meier, P., & Vieweg, S. (2014). Aidr: Artificial intelligence for dis-\\naster response. In Proceedings of the 23rd international conference on world wide web (pp. 159–\\n162). ACM.\\nImran, M., Elbassuoni, S., Castillo, C., Diaz, F., & Meier, P. (2013a). Extracting information nuggets \\nfrom disaster-related messages in social media. In Proceedings of the 10th ISCRAM conference (pp. \\n791–801).\\nImran, M., Elbassuoni, S., Castillo, C., Diaz, F., & Meier, P. (2013b). Practical extraction of disaster-\\nrelevant information from social media. In Proceedings of the 22nd international conference on \\nworld wide web (pp. 1021–1024). ACM.\\nJabbour, C. J. C., Sobreiro, V. A., de Sousa Jabbour, A. B. L., de Souza Campos, L. M., Mariano, E. \\nB., & Renwick, D. W. S. (2017). An analysis of the literature on humanitarian logistics and supply \\nchain management: Paving the way for future studies. Annals of Operations Research. https ://doi.\\norg/10.1007/s1047 9-017-2536-x.\\nJamali, M., Nejat, A., Ghosh, S., Jin, F., & Cao, G. (2019). Social media data and post-disaster recovery. \\nInternational Journal of Information Management, 44, 25–37.\\nJin, S., Jeong, S., Kim, J., & Kim, K. (2015). A logistics model for the transport of disaster victims with \\nvarious injuries and survival probabilities. Annals of Operations Research, 230, 17–33.\\nJohn, L., Gurumurthy, A., Soni, G., & Jain, V. (2018). Modelling the inter-relationship between factors \\naffecting coordination in a humanitarian supply chain: A case of Chennai flood relief. Annals of \\nOperations Research. https ://doi.org/10.1007/s1047 9-018-2963-3.\\nKapoor, K. K., Tamilmani, K., Rana, N. P., Patil, P., Dwivedi, Y. K., & Nerur, S. (2018). Advances in \\nsocial media research: Past, present and future. Information Systems Frontiers, 20(3), 531–558.\\nKim, J., Bae, J., & Hastak, M. (2018). Emergency information diffusion on online social media during \\nstorm Cindy in US. International Journal of Information Management, 40, 153–165.\\nKim, J., & Hastak, M. (2018). Social network analysis: Characteristics of online social networks after a \\ndisaster. International Journal of Information Management, 38(1), 86–96.\\nKingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv  \\n:1412.6980.\\nKumar, A., & Singh, J. P. (2019). Location reference identification from tweets during emergencies: A \\ndeep learning approach. International Journal of Disaster Risk Reduction, 33, 365–375.\\nKumar, A., Singh, J. P., & Rana, N. P. (2017). Authenticity of geo-location and place name in tweets. In \\nProceedings of the 23rd Americas conference on information systems (AMCIS) (pp. 1–9)\\nKumar, A., & Singh, J. P., Saumya, S. (2019). A comparative analysis of machine learning techniques \\nfor disaster related tweet classification. In IEEE region 10 humanitarian technology conference, \\n(pp. 222–227).\\nLagerstrom, R., Arzhaeva, Y., Szul, P., Obst, O., Power, R., Robinson, B., et\\xa0al. (2016). Image classifica-\\ntion to support emergency situation awareness. Frontiers in Robotics and AI, 3, 54.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 30, 'page_label': '821'}, page_content='821Annals of Operations Research (2022) 319:791–822 \\n1 3\\nLi, H., Guevara, N., Herndon, N., Caragea, D., Neppalli, K., Caragea, C., et\\xa0 al. (2015). Twitter min-\\ning for disaster response: A domain adaptation approach. In Proceedings of the 12th ISCRAM \\nconference.\\nMouzannar, H., Rizk, Y., & Awad, M. (2018). Damage identification in social media posts using multi-\\nmodal deep learning. In ISCRAM.\\nNair, V., & Hinton, G. E. (2010). Rectified linear units improve restricted Boltzmann machines. In Pro -\\nceedings of the 27th international conference on machine learning (ICML- 10) (pp. 807–814).\\nNguyen, D. T., Al Mannai, K. A., Joty, S., Sajjad, H., Imran, M., & Mitra, P. (2017a). Robust classifica-\\ntion of crisis-related data on social networks using convolutional neural networks. In 11th interna-\\ntional conference on web and social media, ICWSM 2017 (pp. 632–635). AAAI Press.\\nNguyen, D. T., Alam, F., Ofli, F., & Imran, M. (2017b). Automatic image filtering on social networks \\nusing deep learning and perceptual hashing during crises. arXiv preprint arXiv  :1704.02602 .\\nNguyen, D. T., Joty, S., Imran, M., Sajjad, H., & Mitra, P. (2016). Applications of online deep learning \\nfor crisis response using social media information. arXiv preprint arXiv  :1610.01030 .\\nNguyen, D. T., Ofli, F., Imran, M., & Mitra, P. (2017c). Damage assessment from social media imagery \\ndata during disasters. In Proceedings of the 2017 IEEE/ACM international conference on advances \\nin social networks analysis and mining 2017 (pp. 569–576). ACM.\\nOlteanu, A., Castillo, C., Diaz, F., & Vieweg, S. (2014). Crisislex: A lexicon for collecting and filtering \\nmicroblogged communications in crises. In Eighth international AAAI conference on weblogs and \\nsocial media.\\nPapadopoulos, T., Gunasekaran, A., Dubey, R., Altay, N., Childe, S. J., & Fosso-Wamba, S. (2017). \\nThe role of big data in explaining disaster resilience in supply chains for sustainability. Journal of \\nCleaner Production, 142, 1108–1118.\\nPaul, J. A., & Hariharan, G. (2012). Location-allocation planning of stockpiles for effective disaster mit-\\nigation. Annals of Operations Research, 196, 469–490.\\nPennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors for word representation. In Pro -\\nceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)  \\n(pp. 1532–1543).\\nRagini, J. R., Anand, P. R., & Bhaskar, V. (2018). Big data analytics for disaster response and recovery \\nthrough sentiment analysis. International Journal of Information Management, 42, 13–24.\\nRizk, Y., Jomaa, H. S., Awad, M., & Castillo, C. (2019). A computationally efficient multi-modal clas-\\nsification approach of disaster-related Twitter images. In Proceedings of the 34th ACM/SIGAPP \\nsymposium on applied computing (pp. 2050–2059). ACM.\\nRudra, K., Banerjee, S., Ganguly, N., Goyal, P., Imran, M., & Mitra, P. (2016). Summarizing situational \\ntweets in crisis scenario. In Proceedings of the 27th ACM conference on hypertext and social media  \\n(pp. 137–147). ACM.\\nSakaki, T., Okazaki, M., & Matsuo, Y. (2013). Tweet analysis for real-time event detection and earth-\\nquake reporting system development. IEEE Transactions on Knowledge and Data Engineering, 25, \\n919–931.\\nShareef, M. A., Dwivedi, Y. K., Mahmud, R., Wright, A., Rahman, M. M., Kizgin, H., et\\xa0al. (2018). Dis-\\naster management in Bangladesh: Developing an effective emergency supply chain network. Annals \\nof Operations Research. https ://doi.org/10.1007/s1047 9-018-3081-y.\\nSimonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recogni-\\ntion. arXiv preprint arXiv  :1409.1556.\\nSingh, J. P., Dwivedi, Y. K., Rana, N. P., Kumar, A., & Kapoor, K. K. (2019). Event classification and \\nlocation prediction from tweets during disasters. Annals of Operations Research, 283(1–2), 737–\\n757. https ://doi.org/10.1007/s1047 9-017-2522-3.\\nSinha, A., Kumar, P., Rana, N. P., Islam, R., & Dwivedi, Y. K. (2019). Impact of internet of things (IoT) \\nin disaster management: A task-technology fit perspective. Annals of Operations Research, 283, \\n759–794. https ://doi.org/10.1007/s1047 9-017-2658-1.\\nSon, J., Lee, H. K., Jin, S., & Lee, J. (2019). Content features of tweets for effective communication \\nduring disasters: A media synchronicity theory perspective. International Journal of Information \\nManagement, 45, 56–68.\\nSundermeyer, M., Schlüter, R., & Ney, H. (2012). LSTM neural networks for language modeling. In \\nThirteenth annual conference of the international speech communication association.\\nVerma, S., Vieweg, S., Corvey, W. J., Palen, L., Martin, J. H., Palmer, M., et\\xa0al. (2011). Natural lan-\\nguage processing to the rescue? Extracting” situational awareness” tweets during mass emergency. \\nIn Fifth international AAAI conference on weblogs and social media.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2020-01-16T13:42:25+05:30', 'author': 'Abhinav Kumar', 'crossmarkdomains[1]': 'springer.com', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'crossmarkmajorversiondate': '2010-04-23', 'keywords': 'Disaster; Twitter; LSTM; VGG-16; Social media; Tweets', 'moddate': '2022-11-17T09:09:17+05:30', 'subject': 'Annals of Operations Research, https://doi.org/10.1007/s10479-020-03514-x', 'title': 'A deep multi-modal neural network for informative Twitter content classification during emergencies', 'doi': '10.1007/s10479-020-03514-x', 'robots': 'noindex', 'source': 'Deep_multimodal_neural_network.pdf', 'total_pages': 32, 'page': 31, 'page_label': '822'}, page_content='822 Annals of Operations Research (2022) 319:791–822\\n1 3\\nYu, M., Huang, Q., Qin, H., Scheele, C., & Yang, C. (2019). Deep learning for real-time social media \\ntext classification for situation awareness using hurricanes sandy, Harvey, and Irma as case studies. \\nInternational Journal of Digital Earth. https ://doi.org/10.1080/17538 947.2019.15743 16.\\nZhang, C., Fan, C., Yao, W., Hu, X., & Mostafavi, A. (2019). Social media for intelligent public infor -\\nmation and warning in disasters: An interdisciplinary review. International Journal of Information \\nManagement, 49, 190–207.\\nZheng, X., Han, J., & Sun, A. (2018). A survey of location prediction on Twitter. IEEE Transactions on \\nKnowledge and Data Engineering, 30, 1652–1671.\\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \\ninstitutional affiliations.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reding pdf \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('Deep_multimodal_neural_network.pdf')\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a32e8488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b7564d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader=WebBaseLoader(web_paths=(\"https://python.langchain.com/docs/integrations/document_loaders/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"theme-doc-markdown markdown\")\n",
    "                         ))\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f38c4830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/integrations/document_loaders/'}, page_content='Document loaders\\n\\nDocumentLoaders load data into the standard LangChain Document format.\\nEach DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the .load method.\\nAn example use case is as follows:\\nfrom langchain_community.document_loaders.csv_loader import CSVLoaderloader = CSVLoader(    ...  # <-- Integration specific parameters here)data = loader.load()API Reference:CSVLoader\\nWebpages\\u200b\\nThe below document loaders allow you to load webpages.\\nSee this guide for a starting point: How to: load web pages.\\nDocument LoaderDescriptionPackage/APIWebUses urllib and BeautifulSoup to load and parse HTML web pagesPackageUnstructuredUses Unstructured to load and parse web pagesPackageRecursiveURLRecursively scrapes all child links from a root URLPackageSitemapScrapes all pages on a given sitemapPackageFirecrawlAPI service that can be deployed locally, hosted version has free credits.APIDoclingUses Docling to load and parse web pagesPackageHyperbrowserPlatform for running and scaling headless browsers, can be used to scrape/crawl any siteAPIAgentQLWeb interaction and structured data extraction from any web page using an AgentQL query or a Natural Language promptAPI\\nPDFs\\u200b\\nThe below document loaders allow you to load PDF documents.\\nSee this guide for a starting point: How to: load PDF files.\\nDocument LoaderDescriptionPackage/APIPyPDFUses `pypdf` to load and parse PDFsPackageUnstructuredUses Unstructured\\'s open source library to load PDFsPackageAmazon TextractUses AWS API to load PDFsAPIMathPixUses MathPix to load PDFsPackagePDFPlumberLoad PDF files using PDFPlumberPackagePyPDFDirectryLoad a directory with PDF filesPackagePyPDFium2Load PDF files using PyPDFium2PackagePyMuPDFLoad PDF files using PyMuPDFPackagePyMuPDF4LLMLoad PDF content to Markdown using PyMuPDF4LLMPackagePDFMinerLoad PDF files using PDFMinerPackageUpstage Document Parse LoaderLoad PDF files using UpstageDocumentParseLoaderPackageDoclingLoad PDF files using DoclingPackage\\nCloud Providers\\u200b\\nThe below document loaders allow you to load documents from your favorite cloud providers.\\nDocument LoaderDescriptionPartner PackageAPI referenceAWS S3 DirectoryLoad documents from an AWS S3 directory❌S3DirectoryLoaderAWS S3 FileLoad documents from an AWS S3 file❌S3FileLoaderAzure AI DataLoad documents from Azure AI services❌AzureAIDataLoaderAzure Blob Storage ContainerLoad documents from an Azure Blob Storage container❌AzureBlobStorageContainerLoaderAzure Blob Storage FileLoad documents from an Azure Blob Storage file❌AzureBlobStorageFileLoaderDropboxLoad documents from Dropbox❌DropboxLoaderGoogle Cloud Storage DirectoryLoad documents from GCS bucket✅GCSDirectoryLoaderGoogle Cloud Storage FileLoad documents from GCS file object✅GCSFileLoaderGoogle DriveLoad documents from Google Drive (Google Docs only)✅GoogleDriveLoaderHuawei OBS DirectoryLoad documents from Huawei Object Storage Service Directory❌OBSDirectoryLoaderHuawei OBS FileLoad documents from Huawei Object Storage Service File❌OBSFileLoaderMicrosoft OneDriveLoad documents from Microsoft OneDrive❌OneDriveLoaderMicrosoft SharePointLoad documents from Microsoft SharePoint❌SharePointLoaderTencent COS DirectoryLoad documents from Tencent Cloud Object Storage Directory❌TencentCOSDirectoryLoaderTencent COS FileLoad documents from Tencent Cloud Object Storage File❌TencentCOSFileLoader\\nSocial Platforms\\u200b\\nThe below document loaders allow you to load documents from different social media platforms.\\nDocument LoaderAPI referenceTwitterTwitterTweetLoaderRedditRedditPostsLoader\\nMessaging Services\\u200b\\nThe below document loaders allow you to load data from different messaging platforms.\\nDocument LoaderAPI referenceTelegramTelegramChatFileLoaderWhatsAppWhatsAppChatLoaderDiscordDiscordChatLoaderFacebook ChatFacebookChatLoaderMastodonMastodonTootsLoader\\nProductivity tools\\u200b\\nThe below document loaders allow you to load data from commonly used productivity tools.\\nDocument LoaderAPI referenceFigmaFigmaFileLoaderNotionNotionDirectoryLoaderSlackSlackDirectoryLoaderQuipQuipLoaderTrelloTrelloLoaderRoamRoamLoaderGitHubGithubFileLoader\\nCommon File Types\\u200b\\nThe below document loaders allow you to load data from common data formats.\\nDocument LoaderData TypeCSVLoaderCSV filesDirectoryLoaderAll files in a given directoryUnstructuredMany file types (see https://docs.unstructured.io/platform/supported-file-types)JSONLoaderJSON filesBSHTMLLoaderHTML filesDoclingLoaderVarious file types (see https://ds4sd.github.io/docling/)\\nAll document loaders\\u200b\\nNameDescriptionacreomacreom is a dev-first knowledge base with tasks running on local mark...AgentQLLoaderAgentQL\\'s document loader provides structured data extraction from an...AirbyteLoaderAirbyte is a data integration platform for ELT pipelines from APIs, d...Airtable* Get your API key here.Alibaba Cloud MaxComputeAlibaba Cloud MaxCompute (previously known as ODPS) is a general purp...Amazon TextractAmazon Textract is a machine learning (ML) service that automatically...Apify DatasetApify Dataset is a scalable append-only storage with sequential acces...ArcGISThis notebook demonstrates the use of the langchaincommunity.document...ArxivLoaderarXiv is an open-access archive for 2 million scholarly articles in t...AssemblyAI Audio TranscriptsThe AssemblyAIAudioTranscriptLoader allows to transcribe audio files ...AstraDBDataStax Astra DB is a serverlessAsync ChromiumChromium is one of the browsers supported by Playwright, a library us...AsyncHtmlAsyncHtmlLoader loads raw HTML from a list of URLs concurrently.AthenaAmazon Athena is a serverless, interactive analytics service builtAWS S3 DirectoryAmazon Simple Storage Service (Amazon S3) is an object storage serviceAWS S3 FileAmazon Simple Storage Service (Amazon S3) is an object storage servic...AZLyricsAZLyrics is a large, legal, every day growing collection of lyrics.Azure AI DataAzure AI Studio provides the capability to upload data assets to clou...Azure Blob Storage ContainerAzure Blob Storage is Microsoft\\'s object storage solution for the clo...Azure Blob Storage FileAzure Files offers fully managed file shares in the cloud that are ac...Azure AI Document IntelligenceAzure AI Document Intelligence (formerly known as Azure Form Recogniz...BibTeXBibTeX is a file format and reference management system commonly used...BiliBiliBilibili is one of the most beloved long-form video sites in China.BlackboardBlackboard Learn (previously the Blackboard Learning Management Syste...BlockchainThe intention of this notebook is to provide a means of testing funct...BoxThe langchain-box package provides two methods to index your files fr...Brave SearchBrave Search is a search engine developed by Brave Software.BrowserbaseBrowserbase is a developer platform to reliably run, manage, and moni...BrowserlessBrowserless is a service that allows you to run headless Chrome insta...BSHTMLLoaderThis notebook provides a quick overview for getting started with Beau...CassandraCassandra is a NoSQL, row-oriented, highly scalable and highly availa...ChatGPT DataChatGPT is an artificial intelligence (AI) chatbot developed by OpenA...College ConfidentialCollege Confidential gives information on 3,800+ colleges and univers...Concurrent LoaderWorks just like the GenericLoader but concurrently for those who choo...ConfluenceConfluence is a wiki collaboration platform designed to save and orga...CoNLL-UCoNLL-U is revised version of the CoNLL-X format. Annotations are enc...Copy PasteThis notebook covers how to load a document object from something you...CouchbaseCouchbase is an award-winning distributed NoSQL cloud database that d...CSVA comma-separated values (CSV) file is a delimited text file that use...Cube Semantic LayerThis notebook demonstrates the process of retrieving Cube\\'s data mode...Datadog LogsDatadog is a monitoring and analytics platform for cloud-scale applic...DedocThis sample demonstrates the use of Dedoc in combination with LangCha...DiffbotDiffbot is a suite of ML-based products that make it easy to structur...DiscordDiscord is a VoIP and instant messaging social platform. Users have t...DoclingDocling parses PDF, DOCX, PPTX, HTML, and other formats into a rich u...DocugamiThis notebook covers how to load documents from Docugami. It provides...DocusaurusDocusaurus is a static-site generator which provides out-of-the-box d...DropboxDropbox is a file hosting service that brings everything-traditional ...DuckDBDuckDB is an in-process SQL OLAP database management system.EmailThis notebook shows how to load email (.eml) or Microsoft Outlook (.m...EPubEPUB is an e-book file format that uses the \".epub\" file extension. T...EtherscanEtherscan  is the leading blockchain explorer, search, API and analyt...EverNoteEverNote is intended for archiving and creating notes in which photos...example_dataFacebook ChatMessenger) is an American proprietary instant messaging app and platf...FaunaFauna is a Document Database.FigmaFigma is a collaborative web application for interface design.FireCrawlFireCrawl crawls and convert any website into LLM-ready data. It craw...GeopandasGeopandas is an open-source project to make working with geospatial d...GitGit is a distributed version control system that tracks changes in an...GitBookGitBook is a modern documentation platform where teams can document e...GitHubThis notebooks shows how you can load issues and pull requests (PRs) ...Glue CatalogThe AWS Glue Data Catalog is a centralized metadata repository that a...Google AlloyDB for PostgreSQLAlloyDB is a fully managed relational database service that offers hi...Google BigQueryGoogle BigQuery is a serverless and cost-effective enterprise data wa...Google BigtableBigtable is a key-value and wide-column store, ideal for fast access ...Google Cloud SQL for SQL serverCloud SQL is a fully managed relational database service that offers ...Google Cloud SQL for MySQLCloud SQL is a fully managed relational database service that offers ...Google Cloud SQL for PostgreSQLCloud SQL for PostgreSQL is a fully-managed database service that hel...Google Cloud Storage DirectoryGoogle Cloud Storage is a managed service for storing unstructured da...Google Cloud Storage FileGoogle Cloud Storage is a managed service for storing unstructured da...Google Firestore in Datastore ModeFirestore in Datastore Mode is a NoSQL document database built for au...Google DriveGoogle Drive is a file storage and synchronization service developed ...Google El Carro for Oracle WorkloadsGoogle El Carro Oracle OperatorGoogle Firestore (Native Mode)Firestore is a serverless document-oriented database that scales to m...Google Memorystore for RedisGoogle Memorystore for Redis is a fully-managed service that is power...Google SpannerSpanner is a highly scalable database that combines unlimited scalabi...Google Speech-to-Text Audio TranscriptsThe SpeechToTextLoader allows to transcribe audio files with the Goog...GrobidGROBID is a machine learning library for extracting, parsing, and re-...GutenbergProject Gutenberg is an online library of free eBooks.Hacker NewsHacker News (sometimes abbreviated as HN) is a social news website fo...Huawei OBS DirectoryThe following code demonstrates how to load objects from the Huawei O...Huawei OBS FileThe following code demonstrates how to load an object from the Huawei...HuggingFace datasetThe Hugging Face Hub is home to over 5,000 datasets in more than 100 ...HyperbrowserLoaderHyperbrowser is a platform for running and scaling headless browsers....iFixitiFixit is the largest, open repair community on the web. The site con...ImagesThis covers how to load images into a document format that we can use...Image captionsBy default, the loader utilizes the pre-trained Salesforce BLIP image...IMSDbIMSDb is the Internet Movie Script Database.IuguIugu is a Brazilian services and software as a service (SaaS) company...JoplinJoplin is an open-source note-taking app. Capture your thoughts and s...JSONLoaderThis notebook provides a quick overview for getting started with JSON...Jupyter NotebookJupyter Notebook (formerly IPython Notebook) is a web-based interacti...KineticaThis notebooks goes over how to load documents from KineticalakeFSlakeFS provides scalable version control over the data lake, and uses...LangSmithThis notebook provides a quick overview for getting started with the ...LarkSuite (FeiShu)LarkSuite is an enterprise collaboration platform developed by ByteDa...LLM SherpaThis notebook covers how to use LLM Sherpa to load files of many type...MastodonMastodon is a federated social media and social networking service.MathPixPDFLoaderInspired by Daniel Gross\\'s snippet here//gist.github.com/danielgross/...MediaWiki DumpMediaWiki XML Dumps contain the content of a wiki (wiki pages with al...Merge Documents LoaderMerge the documents returned from a set of specified data loaders.mhtmlMHTML is a is used both for emails but also for archived webpages. MH...Microsoft ExcelThe UnstructuredExcelLoader is used to load Microsoft Excel files. Th...Microsoft OneDriveMicrosoft OneDrive (formerly SkyDrive) is a file hosting service oper...Microsoft OneNoteThis notebook covers how to load documents from OneNote.Microsoft PowerPointMicrosoft PowerPoint is a presentation program by Microsoft.Microsoft SharePointMicrosoft SharePoint is a website-based collaboration system that use...Microsoft WordMicrosoft Word is a word processor developed by Microsoft.Near BlockchainThe intention of this notebook is to provide a means of testing funct...Modern TreasuryModern Treasury simplifies complex payment operations. It is a unifie...MongoDBMongoDB is a NoSQL , document-oriented database that supports JSON-li...Needle Document LoaderNeedle makes it easy to create your RAG pipelines with minimal effort.News URLThis covers how to load HTML news articles from a list of URLs into a...Notion DB 2/2Notion is a collaboration platform with modified Markdown support tha...NucliaNuclia automatically indexes your unstructured data from any internal...ObsidianObsidian is a powerful and extensible knowledge baseOpen Document Format (ODT)The Open Document Format for Office Applications (ODF), also known as...Open City DataSocrata provides an API for city open data.Oracle Autonomous DatabaseOracle autonomous database is a cloud database that uses machine lear...Oracle AI Vector Search: Document ProcessingOracle AI Vector Search is designed for Artificial Intelligence (AI) ...Org-modeA Org Mode document is a document editing, formatting, and organizing...Outline Document LoaderOutline is an open-source collaborative knowledge base platform desig...Pandas DataFrameThis notebook goes over how to load data from a pandas DataFrame.parsersPDFMinerLoaderThis notebook provides a quick overview for getting started with PDFM...PDFPlumberLike PyMuPDF, the output Documents contain detailed metadata about th...Pebblo Safe DocumentLoaderPebblo enables developers to safely load data and promote their Gen A...Polars DataFrameThis notebook goes over how to load data from a polars DataFrame.Dell PowerScale Document LoaderDell PowerScale is an enterprise scale out storage system that hosts ...PsychicThis notebook covers how to load documents from Psychic. See here for...PubMedPubMed® by The National Center for Biotechnology Information, Nationa...PullMdLoaderLoader for converting URLs into Markdown using the pull.md service.PyMuPDFLoaderThis notebook provides a quick overview for getting started with PyMu...PyMuPDF4LLMThis notebook provides a quick overview for getting started with PyMu...PyPDFDirectoryLoaderThis loader loads all PDF files from a specific directory.PyPDFium2LoaderThis notebook provides a quick overview for getting started with PyPD...PyPDFLoaderThis notebook provides a quick overview for getting started with PyPD...PySparkThis notebook goes over how to load data from a PySpark DataFrame.QuipQuip is a collaborative productivity software suite for mobile and We...ReadTheDocs DocumentationRead the Docs is an open-sourced free software documentation hosting ...Recursive URLThe RecursiveUrlLoader lets you recursively scrape all child links fr...RedditReddit is an American social news aggregation, content rating, and di...RoamROAM is a note-taking tool for networked thought, designed to create ...Rockset⚠️ Deprecation Notice: Rockset Integration DisabledrspaceThis notebook shows how to use the RSpace document loader to import r...RSS FeedsThis covers how to load HTML news articles from a list of RSS feed UR...RSTA reStructured Text (RST) file is a file format for textual data used...scrapflyScrapFly is a web scraping API with headless browser capabilities, pr...ScrapingAntScrapingAnt is a web scraping API with headless browser capabilities,...SingleStoreThe SingleStoreLoader allows you to load documents directly from a Si...SitemapExtends from the WebBaseLoader, SitemapLoader loads a sitemap from a ...SlackSlack is an instant messaging program.SnowflakeThis notebooks goes over how to load documents from SnowflakeSource CodeThis notebook covers how to load source code files using a special ap...SpiderSpider is the fastest and most affordable crawler and scraper that re...SpreedlySpreedly is a service that allows you to securely store credit cards ...StripeStripe is an Irish-American financial services and software as a serv...SubtitleThe SubRip file format is described on the Matroska multimedia contai...SurrealDBSurrealDB is an end-to-end cloud-native database designed for modern ...TelegramTelegram Messenger is a globally accessible freemium, cross-platform,...Tencent COS DirectoryTencent Cloud Object Storage (COS) is a distributedTencent COS FileTencent Cloud Object Storage (COS) is a distributedTensorFlow DatasetsTensorFlow Datasets is a collection of datasets ready to use, with Te...TiDBTiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution...2Markdown2markdown service transforms website content into structured markdown...TOMLTOML is a file format for configuration files. It is intended to be e...TrelloTrello is a web-based project management and collaboration tool that ...TSVA tab-separated values (TSV) file is a simple, text-based file format...TwitterTwitter is an online social media and social networking service.UnstructuredThis notebook covers how to use Unstructured document loader to load ...UnstructuredMarkdownLoaderThis notebook provides a quick overview for getting started with Unst...UnstructuredPDFLoaderUnstructured supports a common interface for working with unstructure...UpstageThis notebook covers how to get started with UpstageDocumentParseLoad...URLThis example covers how to load HTML documents from a list of URLs in...VsdxA visio file (with extension .vsdx) is associated with Microsoft Visi...WeatherOpenWeatherMap is an open-source weather service providerWebBaseLoaderThis covers how to use WebBaseLoader to load all text from HTML webpa...WhatsApp ChatWhatsApp (also called WhatsApp Messenger) is a freeware, cross-platfo...WikipediaWikipedia is a multilingual free online encyclopedia written and main...UnstructuredXMLLoaderThis notebook provides a quick overview for getting started with Unst...Xorbits Pandas DataFrameThis notebook goes over how to load data from a xorbits.pandas DataFr...YouTube audioBuilding chat or QA applications on YouTube videos is a topic of high...YouTube transcriptsYouTube is an online video sharing and social media platform created ...YoutubeLoaderDLLoader for Youtube leveraging the yt-dlp library.YuqueYuque is a professional cloud-based knowledge base for team collabora...ZeroxPDFLoaderZeroxPDFLoader is a document loader that leverages the Zerox library....')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d0a5d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs=ArxivLoader(query=\"1706.03762 \").load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37d8c67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae3334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take document from Wikipedia\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "docs = WikipediaLoader(query=\"India\", load_max_docs=2).load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91fb7f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'India', 'summary': \"India, officially the Republic of India, is a country in South Asia.  It is the seventh-largest country by area; the most populous country since 2023; and, since its independence in 1947, the world's most populous democracy. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is near Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand, Myanmar, and Indonesia.\\nModern humans arrived on the Indian subcontinent from Africa no later than 55,000 years ago. Their long occupation, predominantly in isolation as hunter-gatherers, has made the region highly diverse. Settled life emerged on the subcontinent in the western margins of the Indus river basin 9,000 years ago, evolving gradually into the Indus Valley Civilisation of the third millennium BCE. By 1200 BCE, an archaic form of Sanskrit, an Indo-European language, had diffused into India from the northwest. Its hymns recorded the early dawnings of Hinduism in India. India's pre-existing Dravidian languages were supplanted in the northern regions. By 400 BCE, caste had emerged within Hinduism, and Buddhism and Jainism had arisen, proclaiming social orders unlinked to heredity. Early political consolidations gave rise to the loose-knit Maurya and Gupta Empires. Widespread creativity suffused this era, but the status of women declined, and untouchability became an organized belief. In South India, the Middle kingdoms exported Dravidian language scripts and religious cultures to the kingdoms of Southeast Asia.\\nIn the early medieval era, Christianity, Islam, Judaism, and Zoroastrianism became established on India's southern and western coasts. Muslim armies from Central Asia intermittently overran India's northern plains in the second millennium. The resulting Delhi Sultanate drew northern India into the cosmopolitan networks of medieval Islam. In south India, the Vijayanagara Empire created a long-lasting composite Hindu culture. In the Punjab, Sikhism emerged, rejecting institutionalised religion. The Mughal Empire ushered in two centuries of economic expansion and relative peace, leaving a rich architectural legacy. Gradually expanding rule of the British East India Company turned India into a colonial economy but consolidated its sovereignty. British Crown rule began in 1858. The rights promised to Indians were granted slowly, but technological changes were introduced, and modern ideas of education and the public life took root. A nationalist movement emerged in India, the first in the non-European British empire and an influence on other nationalist movements.  Noted for nonviolent resistance after 1920, it became the primary factor in ending British rule. In 1947, the British Indian Empire was partitioned into two independent dominions, a Hindu-majority dominion of India and a Muslim-majority dominion of Pakistan. A large-scale loss of life and an unprecedented migration accompanied the partition.\\nIndia has been a federal republic since 1950, governed through a democratic parliamentary system. It is a pluralistic, multilingual and multi-ethnic society. India's population grew from 361 million in 1951 to over 1.4 billion in 2023. During this time, its nominal per capita income increased from US$64 annually to US$2,601, and its literacy rate from 16.6% to 74%. A comparatively destitute country in 1951, India has become a fast-growing major economy and hub for information technology services; it has an expanding middle class. Indian movies and music increasingly influence global culture. India has reduced its poverty rate, though at the cost of increasing economic inequality. It is a nuclear-weapon state that ranks high in military expenditure. It has disputes over Kashmir with its neighbours, Pakistan and China, unresolved since the mid-20th century. Among the socio-economic challenges India faces are gender inequality, child malnutrition, and rising levels of air pollution. India's land is megadiverse with four biodiversity hotspots. India's wildlife, which has traditionally been viewed with tolerance in its culture, is supported in protected habitats.\", 'source': 'https://en.wikipedia.org/wiki/India'}, page_content=\"India, officially the Republic of India, is a country in South Asia.  It is the seventh-largest country by area; the most populous country since 2023; and, since its independence in 1947, the world's most populous democracy. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is near Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand, Myanmar, and Indonesia.\\nModern humans arrived on the Indian subcontinent from Africa no later than 55,000 years ago. Their long occupation, predominantly in isolation as hunter-gatherers, has made the region highly diverse. Settled life emerged on the subcontinent in the western margins of the Indus river basin 9,000 years ago, evolving gradually into the Indus Valley Civilisation of the third millennium BCE. By 1200 BCE, an archaic form of Sanskrit, an Indo-European language, had diffused into India from the northwest. Its hymns recorded the early dawnings of Hinduism in India. India's pre-existing Dravidian languages were supplanted in the northern regions. By 400 BCE, caste had emerged within Hinduism, and Buddhism and Jainism had arisen, proclaiming social orders unlinked to heredity. Early political consolidations gave rise to the loose-knit Maurya and Gupta Empires. Widespread creativity suffused this era, but the status of women declined, and untouchability became an organized belief. In South India, the Middle kingdoms exported Dravidian language scripts and religious cultures to the kingdoms of Southeast Asia.\\nIn the early medieval era, Christianity, Islam, Judaism, and Zoroastrianism became established on India's southern and western coasts. Muslim armies from Central Asia intermittently overran India's northern plains in the second millennium. The resulting Delhi Sultanate drew northern India into the cosmopolitan networks of medieval Islam. In south India, the Vijayanagara Empire created a long-lasting composite Hindu culture. In the Punjab, Sikhism emerged, rejecting institutionalised religion. The Mughal Empire ushered in two centuries of economic expansion and relative peace, leaving a rich architectural legacy. Gradually expanding rule of the British East India Company turned India into a colonial economy but consolidated its sovereignty. British Crown rule began in 1858. The rights promised to Indians were granted slowly, but technological changes were introduced, and modern ideas of education and the public life took root. A nationalist movement emerged in India, the first in the non-European British empire and an influence on other nationalist movements.  Noted for nonviolent resistance after 1920, it became the primary factor in ending British rule. In 1947, the British Indian Empire was partitioned into two independent dominions, a Hindu-majority dominion of India and a Muslim-majority dominion of Pakistan. A large-scale loss of life and an unprecedented migration accompanied the partition.\\nIndia has been a federal republic since 1950, governed through a democratic parliamentary system. It is a pluralistic, multilingual and multi-ethnic society. India's population grew from 361 million in 1951 to over 1.4 billion in 2023. During this time, its nominal per capita income increased from US$64 annually to US$2,601, and its literacy rate from 16.6% to 74%. A comparatively destitute country in 1951, India has become a fast-growing major economy and hub for information technology services; it has an expanding middle class. Indian movies and music increasingly influence global culture. India has reduced its poverty rate, though at the cost of increasing economic inequality. It is a nuclear-weapon state that ranks high in military expenditure. It has disputes over Kashmir with its neighbours, Pakistan and China, unresolved\"),\n",
       " Document(metadata={'title': 'Indian National Developmental Inclusive Alliance', 'summary': \"The Indian National Developmental Inclusive Alliance (INDIA) is a big tent multi-party political alliance of several political parties in India led by the country's largest opposition party, the Indian National Congress. The alliance is in opposition to the ruling National Democratic Alliance (NDA) government led by the Bharatiya Janata Party (BJP) in the 2024 Indian general elections. In the 2024 general election, the alliance won 234 seats, gaining more than 100 seats in relation to its size before dissolution, and the majority of seats in states like Uttar Pradesh, Maharashtra, and West Bengal.\", 'source': 'https://en.wikipedia.org/wiki/Indian_National_Developmental_Inclusive_Alliance'}, page_content='The Indian National Developmental Inclusive Alliance (INDIA) is a big tent multi-party political alliance of several political parties in India led by the country\\'s largest opposition party, the Indian National Congress. The alliance is in opposition to the ruling National Democratic Alliance (NDA) government led by the Bharatiya Janata Party (BJP) in the 2024 Indian general elections. In the 2024 general election, the alliance won 234 seats, gaining more than 100 seats in relation to its size before dissolution, and the majority of seats in states like Uttar Pradesh, Maharashtra, and West Bengal.\\n\\n\\n== Etymology ==\\nThe Indian National Developmental Inclusive Alliance, commonly known by its backronym I.N.D.I.A. is an opposition front announced by the leaders of 28 parties to contest the 2024 Lok Sabha elections. The name was proposed during a meeting in Bengaluru and was unanimously adopted by the 28 participating parties. While some sources attribute the suggestion of the name to Rahul Gandhi, the leader of the Indian National Congress (INC), others mention that it was suggested by Mamata Banerjee, the Trinamool Congress (TMC) supremo and chief minister of West Bengal.\\n\\n\\n== History ==\\nOn September 25, 2022, Indian National Lok Dal (INLD) supremo Om Prakash Chautala hosted a rally in Fatehabad on the occasion of Former Deputy Prime Minister Devi Lal\\'s birth anniversary. During this rally, the ideas of a national alliance were first openly called on stage. Chief Minister of Bihar Nitish Kumar had said \"I\\'ll urge all parties, including Congress, to get together and then they (BJP) will lost badly\". JD(U) spokesperson K. C. Tyagi had also stated that the foundation of the alliance was during this same rally.\\nThe first major Opposition parties\\' meeting, held in Patna, Bihar, was chaired by Nitish Kumar on 23 June 2023, when the proposal for a new alliance was put on the table. The meeting was attended by 16 Opposition parties. INLD was not included in this meeting.\\nThe second meeting, was held in Bengaluru, Karnataka on 17–18 July. It was chaired by UPA Chairperson Sonia Gandhi when the proposal for an alliance was accepted and ten more parties were added to the list. The alliance\\'s name was finalized and given the name Indian National Developmental Inclusive Alliance.\\nThe third meeting was held in Mumbai, Maharashtra from 31 August to 1 September. The meeting was hosted by Shiv Sena (UBT) President Uddhav Thackeray and saw Sonia Gandhi, Rahul Gandhi, and chief ministers of 5 states in attendance. Over the two-day deliberations, the alliance discussed major electoral issues for the upcoming general elections, carved out the coordination committee, and passed a three-point resolution to fight 2024 Indian general elections together \\'as far as possible\\'.\\nThe fourth meeting was held in New Delhi on 19 December. The meeting was primarily held to discuss seat-sharing, joint rallies, and the prime ministerial face and/or convenor of the alliance. The alliance adopted a resolution to ensure maximum use of VVPATs in upcoming elections. \"To enhance confidence in elections, VVPAT slips should be directly given to voters to self-verify and place in a separate box, instead of them falling into the main box. Eventually, all VVPAT slips must be 100% counted, ensuring truly free and fair elections,\" read the resolution passed by the alliance at the meeting. Seat sharing was also to be done by either 31 December 2023 or mid-January 2024. It was also decided that protests will be held across the country on 22 December 2023 against the suspensions of opposition MPs in the Indian Parliament. Some leaders said that the alliance would hold a grand joint rally at Patna on 30 January 2024, the death anniversary of Mahatma Gandhi, although this was not officially announced.\\nThe alliance held its 5th meeting virtually with some leaders not attending. Following the meeting, the Indian National Congress President Mallikarjun Kharge was declared the alliance c')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
